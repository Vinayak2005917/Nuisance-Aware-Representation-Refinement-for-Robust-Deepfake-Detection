{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4b500e",
   "metadata": {},
   "source": [
    "# NARR Deepfake Detector (Benchmark Notebook)\n",
    "\n",
    "This notebook trains/evaluates the **NARR** model (nuisance-aware representation refinement) on FF++ and optionally tests robustness/cross-dataset generalization.\n",
    "\n",
    "**Run order (typical):**\n",
    "1. Imports + config\n",
    "2. Dataset / augmentations\n",
    "3. Model + losses\n",
    "4. (Optional) training loop\n",
    "5. Load best weights\n",
    "6. Evaluation blocks (FF++, JPEG, DFDC, CelebDF)\n",
    "7. Params/FLOPs report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b39b5",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842faafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534261e3",
   "metadata": {},
   "source": [
    "## 2. Config & Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ee215",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- `CFG.DATA_ROOT` should point at a folder with `train/`, `val/`, `test/` and each split containing `real/` + `fake/`.\n",
    "- `CFG.WEIGHTS_DIR` is where checkpoints are saved/loaded from.\n",
    "- Most blocks below are written so you can **toggle training/evaluation** by changing `EPOCHS` and `NUM_RUNS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e6c88c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    SEED = 42\n",
    "    IMG_SIZE = 224\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_WORKERS = 0\n",
    "    LR = 1e-4\n",
    "\n",
    "    LAMBDA_INV = 0.05\n",
    "    LAMBDA_DOM = 0.1\n",
    "\n",
    "    DATA_ROOT = \"FFPP_CViT\"\n",
    "    WEIGHTS_DIR = \"weights\"\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(CFG.SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d13f5",
   "metadata": {},
   "source": [
    "## 3. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa2986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for label, cls in enumerate([\"real\", \"fake\"]):\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            for f in os.listdir(cls_dir):\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.samples.append((os.path.join(cls_dir, f), label))\n",
    "\n",
    "        #print(f\"[Dataset] Loaded {len(self.samples)} samples from {root}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b9b1c",
   "metadata": {},
   "source": [
    "## 4. Augmentations & Corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a9f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JPEGCompression:\n",
    "    def __init__(self, quality):\n",
    "        self.quality = quality\n",
    "\n",
    "    def __call__(self, img):\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"JPEG\", quality=self.quality)\n",
    "        buffer.seek(0)\n",
    "        return Image.open(buffer).convert(\"RGB\")\n",
    "\n",
    "\n",
    "class RandomGamma:\n",
    "    def __init__(self, gamma_range=(0.7, 1.5), p=0.5):\n",
    "        self.gamma_range = gamma_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            gamma = random.uniform(*self.gamma_range)\n",
    "            return transforms.functional.adjust_gamma(img, gamma)\n",
    "        return img\n",
    "\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "    transforms.RandomAffine(2, translate=(0.02, 0.02), scale=(0.95, 1.05), shear=2),\n",
    "\n",
    "    transforms.ColorJitter(0.6, 0.6, 0.6, 0.15),\n",
    "\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))\n",
    "    ], p=0.3),\n",
    "\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    RandomGamma(p=0.5),\n",
    "\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAdjustSharpness(0.5)\n",
    "    ], p=0.3),\n",
    "\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "def build_jpeg_tfms(q):\n",
    "    return transforms.Compose([\n",
    "        JPEGCompression(q),\n",
    "        transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c25f7",
   "metadata": {},
   "source": [
    "## 5. Corruption Functions (Training Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b932b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_image(x):\n",
    "    out = x.clone()\n",
    "\n",
    "    # spatial degradation\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        out = F.interpolate(out, scale_factor=0.75, mode=\"bilinear\", align_corners=False)\n",
    "        out = F.interpolate(out, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # noise\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        out = torch.clamp(out + 0.03 * torch.randn_like(out), 0, 1)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def freq_mix(x, alpha=0.15):\n",
    "    fft = torch.fft.fft2(x)\n",
    "    mag, phase = torch.abs(fft), torch.angle(fft)\n",
    "\n",
    "    mag = mag * (1 + alpha * torch.randn_like(mag))\n",
    "\n",
    "    return torch.real(\n",
    "        torch.fft.ifft2(mag * torch.exp(1j * phase))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d27f91",
   "metadata": {},
   "source": [
    "## 6. Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37153573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = models.resnet34(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.out_channels = 512\n",
    "\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "class MultiScaleNuisanceEstimator(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        c = channels // 4\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels, c, 1)\n",
    "        self.conv3 = nn.Conv2d(channels, c, 3, padding=2, dilation=2)\n",
    "        self.conv5 = nn.Conv2d(channels, c, 3, padding=4, dilation=4)\n",
    "\n",
    "        self.proj = nn.Conv2d(3 * c, channels, 1)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = torch.cat([self.conv1(x), self.conv3(x), self.conv5(x)], dim=1)\n",
    "        return self.act(self.proj(f))\n",
    "\n",
    "class GradReverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, λ):\n",
    "        ctx.λ = λ\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        return -ctx.λ * grad, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f3eb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NARR(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nuisance = MultiScaleNuisanceEstimator(channels)\n",
    "\n",
    "        self.gate_c = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.gate_s = nn.Sequential(\n",
    "            nn.Conv2d(channels, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.3))\n",
    "        self.beta = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "        self.domain_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, F, lambda_grl=0.0):\n",
    "        # ----- Nuisance estimation -----\n",
    "        N_hat = self.nuisance(F)\n",
    "\n",
    "        # ----- Gates -----\n",
    "        Gc = self.gate_c(N_hat)          # [B, C, 1, 1]\n",
    "        Gs = self.gate_s(N_hat)          # [B, 1, H, W]\n",
    "        G  = Gc * Gs                     # [B, C, H, W]\n",
    "\n",
    "        alpha = torch.clamp(self.alpha, 0.0, 1.0)\n",
    "        beta  = torch.clamp(self.beta,  0.0, 1.0)\n",
    "\n",
    "        # ----- suppression equation -----\n",
    "        F_ref = F * (1 - alpha * G + beta * (1 - G))\n",
    "\n",
    "\n",
    "        # ----- Domain adversarial head -----\n",
    "        dom = None\n",
    "        if lambda_grl > 0:\n",
    "            rev = GradReverse.apply(N_hat, lambda_grl)\n",
    "            dom = self.domain_head(rev)\n",
    "\n",
    "        return F_ref, N_hat, G, dom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac129b56",
   "metadata": {},
   "source": [
    "## 7. Tokenization & Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4735fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHead(nn.Module):\n",
    "    def __init__(self, in_channels, embed_dim=256, num_tokens=8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, 1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((num_tokens, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)                  # [B, D, H, W]\n",
    "        x = self.pool(x)                  # [B, D, N, 1]\n",
    "        return x.squeeze(-1).permute(0, 2, 1)  # [B, N, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecb57827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=512,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, 2)\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)        # [B, N, D]\n",
    "        x = x.mean(dim=1)          # token mean pooling\n",
    "        return self.fc(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a7fb8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = CNNBackbone()\n",
    "        self.narr = NARR(self.backbone.out_channels)\n",
    "        self.embedder = EmbeddingHead(self.backbone.out_channels)\n",
    "        self.classifier = TokenClassifier(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        f_ref, _, _, _ = self.narr(f)\n",
    "        tokens = self.embedder(f_ref)\n",
    "        return self.classifier(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81198f2",
   "metadata": {},
   "source": [
    "## 8. Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41a716e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "domain_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def invariance_contrastive_loss(z1, z2, temp=0.2):\n",
    "    z1 = F.normalize(z1.mean(1), dim=1)\n",
    "    z2 = F.normalize(z2.mean(1), dim=1)\n",
    "    logits = (z1 @ z2.T / temp).clamp(-50, 50)\n",
    "    labels = torch.arange(z1.size(0), device=z1.device)\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd49e71",
   "metadata": {},
   "source": [
    "## 9. Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13aa177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_epoch(loader, model, optimizer):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # ---------- CLEAN FORWARD ----------\n",
    "        f = model.backbone(x)\n",
    "\n",
    "        # NARR forward with GRL enabled (domain adversarial)\n",
    "        f_ref, N_hat, _, dom_clean = model.narr(f, lambda_grl=0.1)\n",
    "        tok_n = model.embedder(N_hat)\n",
    "\n",
    "        # ---------- CORRUPTED VIEW ----------\n",
    "        with torch.no_grad():\n",
    "            if torch.rand(1) < 0.5:\n",
    "                x_corr = corrupt_image(x)\n",
    "            else:\n",
    "                x_corr = freq_mix(x)\n",
    "\n",
    "        f_c = model.backbone(x_corr)\n",
    "        _, N_hat_c, _, dom_corrupt = model.narr(f_c, lambda_grl=0.1)\n",
    "        tok_n_c = model.embedder(N_hat_c)\n",
    "\n",
    "        # ---------- INVARIANCE LOSS ----------\n",
    "        loss_inv = invariance_contrastive_loss(tok_n, tok_n_c)\n",
    "\n",
    "        # ---------- CLASSIFICATION ----------\n",
    "        tok = model.embedder(f_ref)\n",
    "        logit = model.classifier(tok)\n",
    "        loss_cls = criterion(logit, y)\n",
    "\n",
    "        # ---------- DOMAIN ADVERSARIAL LOSS ----------\n",
    "        dom_y_clean = torch.zeros(x.size(0), dtype=torch.long, device=device)\n",
    "        dom_y_corrupt = torch.ones(x.size(0), dtype=torch.long, device=device)\n",
    "        loss_dom = domain_criterion(dom_clean, dom_y_clean) + domain_criterion(dom_corrupt, dom_y_corrupt)\n",
    "\n",
    "        # ---------- TOTAL ----------\n",
    "        loss = loss_cls + CFG.LAMBDA_INV * loss_inv + CFG.LAMBDA_DOM * loss_dom\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item()\n",
    "\n",
    "    return total / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12413147",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(loader, model, threshold=0.5):\n",
    "    model.eval()\n",
    "    logits, labels = [], []\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        x = x.to(device)\n",
    "        logits.append(model(x).cpu())\n",
    "        labels.append(y)\n",
    "\n",
    "    logits = torch.cat(logits).numpy()\n",
    "    labels = torch.cat(labels).numpy()\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"auc\": roc_auc_score(labels, probs),\n",
    "        \"precision\": precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44952c81",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6506e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(CFG.WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "model = DeepfakeDetector().to(device)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": model.backbone.parameters(),  \"lr\": CFG.LR * 0.2},\n",
    "    {\"params\": model.narr.parameters(),      \"lr\": CFG.LR},\n",
    "    {\"params\": model.embedder.parameters(),  \"lr\": CFG.LR},\n",
    "    {\"params\": model.classifier.parameters(),\"lr\": CFG.LR},\n",
    "])\n",
    "EPOCHS = 5\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS\n",
    ")\n",
    "\n",
    "train_ds = BinaryImageFolder(os.path.join(CFG.DATA_ROOT, \"train\"), train_tfms)\n",
    "val_ds   = BinaryImageFolder(os.path.join(CFG.DATA_ROOT, \"val\"),   eval_tfms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "ema_auc = None\n",
    "ema_decay = 0.8\n",
    "best_ema_auc = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set EPOCHS > 0 to train. Keep at 0 to skip training (useful when you only want to run eval).\n",
    "EPOCHS = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    avg_loss = train_epoch(train_loader, model, optimizer)\n",
    "    val_metrics = evaluate(val_loader, model)\n",
    "    current_auc = val_metrics[\"auc\"]\n",
    "\n",
    "    if ema_auc is None:\n",
    "        ema_auc = current_auc\n",
    "    else:\n",
    "        ema_auc = ema_decay * ema_auc + (1 - ema_decay) * current_auc\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Loss: {avg_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_metrics['acc']:.4f} | \"\n",
    "        f\"AUC: {current_auc:.4f} | \"\n",
    "        f\"EMA-AUC: {ema_auc:.4f} | \"\n",
    "        f\"P: {val_metrics['precision']:.4f} | \"\n",
    "        f\"R: {val_metrics['recall']:.4f} | \"\n",
    "        f\"F1: {val_metrics['f1']:.4f}\"\n",
    "    )\n",
    "\n",
    "    if ema_auc > best_ema_auc:\n",
    "        best_ema_auc = ema_auc\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f\"{CFG.WEIGHTS_DIR}/best_NARR.pt\"\n",
    "        )\n",
    "        print(f\"  ✓ Saved new best model (EMA-AUC={best_ema_auc:.4f})\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430501a4",
   "metadata": {},
   "source": [
    "## 11. Load Best Model (Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47897ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vk200\\AppData\\Local\\Temp\\ipykernel_46488\\758200.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f\"{CFG.WEIGHTS_DIR}/best_NARR.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load a trained checkpoint before running the evaluation blocks below.\n",
    "print(\"Loading best model...\")\n",
    "model.load_state_dict(\n",
    "    torch.load(f\"{CFG.WEIGHTS_DIR}/best_NARR.pt\", map_location=device)\n",
    ")\n",
    "model.eval()\n",
    "print(\"✓ Best model loaded\")\n",
    "\n",
    "# Number of repeated evaluation runs (different seeds) to average metrics over.\n",
    "# Set to 1 for a single run, >1 to reduce variance, or 0 to skip evaluation blocks safely.\n",
    "NUM_RUNS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22f16f",
   "metadata": {},
   "source": [
    "## 12. FF++ Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cb7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FF++ TEST | AVERAGED OVER 3 RUNS =====\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     19\u001b[39m     metrics = evaluate(ffpp_test_loader, model)\n\u001b[32m     20\u001b[39m     all_metrics.append(metrics)\n\u001b[32m     22\u001b[39m avg_metrics = {\n\u001b[32m     23\u001b[39m     k: \u001b[38;5;28msum\u001b[39m(m[k] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m all_metrics) / NUM_RUNS\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     25\u001b[39m }\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m avg_metrics.items():\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk.upper()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(f\"\\n===== FF++ TEST | AVERAGED OVER {NUM_RUNS} RUN(S) =====\")\n",
    "\n",
    "if NUM_RUNS <= 0:\n",
    "    print(\"NUM_RUNS <= 0; skipping FF++ evaluation.\")\n",
    "else:\n",
    "    all_metrics = []\n",
    "\n",
    "    ffpp_test_ds = BinaryImageFolder(\n",
    "        os.path.join(CFG.DATA_ROOT, \"test\"),\n",
    "        eval_tfms\n",
    ")\n",
    "\n",
    "    ffpp_test_loader = DataLoader(\n",
    "        ffpp_test_ds,\n",
    "        batch_size=CFG.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        set_seed(CFG.SEED + run_idx)\n",
    "        metrics = evaluate(ffpp_test_loader, model)\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    avg_metrics = {\n",
    "        k: sum(m[k] for m in all_metrics) / NUM_RUNS\n",
    "        for k in all_metrics[0]\n",
    "    }\n",
    "\n",
    "    for k, v in avg_metrics.items():\n",
    "        print(f\"{k.upper():>10}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3dda53",
   "metadata": {},
   "source": [
    "## 13. JPEG Compression Robustness Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce2a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== JPEG COMPRESSION TEST | AVERAGED OVER 3 RUNS =====\n",
      "\n",
      "--- JPEG Quality 100% ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9379 |  ACC: 0.8726 |  F1: 0.9188\n",
      "\n",
      "--- JPEG Quality 90% ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9348 |  ACC: 0.8835 |  F1: 0.9272\n",
      "\n",
      "--- JPEG Quality 75% ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9214 |  ACC: 0.8036 |  F1: 0.8675\n",
      "\n",
      "--- JPEG Quality 50% ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8888 |  ACC: 0.7130 |  F1: 0.7929\n",
      "\n",
      "--- JPEG Quality 30% ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8533 |  ACC: 0.6320 |  F1: 0.7175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(f\"\\n===== JPEG COMPRESSION TEST | AVERAGED OVER {NUM_RUNS} RUN(S) =====\")\n",
    "\n",
    "if NUM_RUNS <= 0:\n",
    "    print(\"NUM_RUNS <= 0; skipping JPEG robustness evaluation.\")\n",
    "else:\n",
    "    jpeg_qualities = [100, 90, 75, 50, 30]\n",
    "\n",
    "    for q in jpeg_qualities:\n",
    "        print(f\"\\n--- JPEG Quality {q}% ---\")\n",
    "        run_metrics = []\n",
    "\n",
    "        for run_idx in range(NUM_RUNS):\n",
    "            set_seed(CFG.SEED + run_idx)\n",
    "            jpeg_ds = BinaryImageFolder(\n",
    "                os.path.join(CFG.DATA_ROOT, \"test\"),\n",
    "                build_jpeg_tfms(q)\n",
    ")\n",
    "\n",
    "            jpeg_loader = DataLoader(\n",
    "                jpeg_ds,\n",
    "                batch_size=CFG.BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "            metrics = evaluate(jpeg_loader, model)\n",
    "            run_metrics.append(metrics)\n",
    "\n",
    "        avg_auc = sum(m[\"auc\"] for m in run_metrics) / NUM_RUNS\n",
    "        avg_acc = sum(m[\"acc\"] for m in run_metrics) / NUM_RUNS\n",
    "        avg_f1  = sum(m[\"f1\"]  for m in run_metrics) / NUM_RUNS\n",
    "\n",
    "        print(\n",
    "            f\"AUC: {avg_auc:.4f} | \",\n",
    "            f\"ACC: {avg_acc:.4f} | \",\n",
    "            f\"F1: {avg_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712386bc",
   "metadata": {},
   "source": [
    "## 14. DFDC Cross-Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DFDC CROSS-DATASET TEST | AVERAGED OVER 3 RUNS =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ACC: 0.6909\n",
      "       AUC: 0.7052\n",
      " PRECISION: 0.8741\n",
      "    RECALL: 0.7193\n",
      "        F1: 0.7892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(f\"\\n===== DFDC CROSS-DATASET TEST | AVERAGED OVER {NUM_RUNS} RUN(S) =====\")\n",
    "\n",
    "if NUM_RUNS <= 0:\n",
    "    print(\"NUM_RUNS <= 0; skipping DFDC cross-dataset evaluation.\")\n",
    "else:\n",
    "    DFDC_ROOT = \"./DFDC/validation\"\n",
    "\n",
    "    dfdc_ds = BinaryImageFolder(\n",
    "        DFDC_ROOT,\n",
    "        eval_tfms\n",
    ")\n",
    "\n",
    "    dfdc_loader = DataLoader(\n",
    "        dfdc_ds,\n",
    "        batch_size=CFG.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "    all_metrics = []\n",
    "\n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        set_seed(CFG.SEED + run_idx)\n",
    "        metrics = evaluate(dfdc_loader, model, threshold=0.5)\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    avg_metrics = {\n",
    "        k: sum(m[k] for m in all_metrics) / NUM_RUNS\n",
    "        for k in all_metrics[0]\n",
    "    }\n",
    "\n",
    "    for k, v in avg_metrics.items():\n",
    "        print(f\"{k.upper():>10}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065ff26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CELEB-DF CROSS-DATASET (NARR) | AVERAGED OVER 3 RUNS =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vk200\\AppData\\Local\\Temp\\ipykernel_43704\\2029901009.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f\"{CFG.WEIGHTS_DIR}/best_NARR.pt\", map_location=device)\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ACC: 0.6856\n",
      "       AUC: 0.7113\n",
      " PRECISION: 0.7578\n",
      "    RECALL: 0.7660\n",
      "        F1: 0.7619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(f\"\\n===== CELEB-DF CROSS-DATASET (NARR) | AVERAGED OVER {NUM_RUNS} RUN(S) =====\")\n",
    "\n",
    "if NUM_RUNS <= 0:\n",
    "    print(\"NUM_RUNS <= 0; skipping Celeb-DF cross-dataset evaluation.\")\n",
    "else:\n",
    "    CELEBDF_ROOT = \"./CelebDF_images/test\"\n",
    "\n",
    "    celeb_ds = BinaryImageFolder(\n",
    "        CELEBDF_ROOT,\n",
    "        eval_tfms\n",
    ")\n",
    "\n",
    "    celeb_loader = DataLoader(\n",
    "        celeb_ds,\n",
    "        batch_size=CFG.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    ")\n",
    "\n",
    "    # (Re)load best NARR model to be explicit about the weights used for reporting\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"{CFG.WEIGHTS_DIR}/best_NARR.pt\", map_location=device)\n",
    ")\n",
    "    model.eval()\n",
    "\n",
    "    all_metrics = []\n",
    "\n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        set_seed(CFG.SEED + run_idx)\n",
    "        metrics = evaluate(\n",
    "            celeb_loader,\n",
    "            model,\n",
    "            threshold=0.5\n",
    ")\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    avg_metrics = {\n",
    "        k: sum(m[k] for m in all_metrics) / NUM_RUNS\n",
    "        for k in all_metrics[0]\n",
    "}\n",
    "\n",
    "    for k, v in avg_metrics.items():\n",
    "        print(f\"{k.upper():>10}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# COMPUTE PARAMS + FLOPs FOR PAPER\n",
    "# =====================================\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "def compute_model_cost(model, name=\"Model\"):\n",
    "    \"\"\"Compute Params/FLOPs using a single dummy forward pass.\"\"\"\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # Dummy input at training resolution\n",
    "    dummy = torch.randn(1, 3, CFG.IMG_SIZE, CFG.IMG_SIZE).to(device)\n",
    "\n",
    "    # thop returns raw numbers (multiply-adds convention depends on ops)\n",
    "    flops, params = profile(\n",
    "        model,\n",
    "        inputs=(dummy,),\n",
    "        verbose=False\n",
    ")\n",
    "\n",
    "    # Pretty formatting (K, M, G)\n",
    "    flops_str, params_str = clever_format([flops, params], \"%.3f\")\n",
    "\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(f\"Params : {params_str}\")\n",
    "    print(f\"FLOPs  : {flops_str}\")\n",
    "\n",
    "    return flops, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e7346",
   "metadata": {},
   "source": [
    "## 15. Model Cost (Params + FLOPs)\n",
    "Uses `thop` to estimate compute cost with a dummy input at `CFG.IMG_SIZE`. If you haven’t installed it yet: `pip install thop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f9847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== NARR Detector =====\n",
      "Params : 23.650M\n",
      "FLOPs  : 3.760G\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a fresh model for cost reporting (weights are not required for Params/FLOPs).\n",
    "narr_model = DeepfakeDetector().to(device)\n",
    "\n",
    "# Compute costs\n",
    "flops_narr, params_narr = compute_model_cost(\n",
    "    narr_model,\n",
    "    name=\"NARR Detector\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
