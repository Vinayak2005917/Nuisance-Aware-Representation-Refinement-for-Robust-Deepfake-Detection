{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1b39b5",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad80c72",
   "metadata": {},
   "source": [
    "# Nuisance Aware Representation Refinement (NARR) for Robust Deepfake Detection\n",
    "\n",
    "This notebook implements the NARR model, a novel approach for deepfake detection that focuses on refining feature representations by estimating and mitigating nuisance factors that affect detection robustness.\n",
    "\n",
    "## Overview\n",
    "\n",
    "NARR addresses the challenge of deepfake detection under various corruptions and cross-dataset scenarios by:\n",
    "\n",
    "1. **Nuisance Estimation**: Learning to identify nuisance factors in feature representations\n",
    "2. **Adaptive Refinement**: Using learned gates to suppress nuisance while preserving discriminative features\n",
    "3. **Domain Adversarial Training**: Improving generalization across different data distributions\n",
    "4. **Contrastive Invariance**: Ensuring robustness to image corruptions\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **CNN Backbone**: ResNet-34 feature extractor\n",
    "- **Multi-Scale Nuisance Estimator**: Estimates nuisance at multiple scales\n",
    "- **Adaptive Gates**: Channel-wise and spatial gating for feature refinement\n",
    "- **Token-based Classifier**: Transformer-based classification with token pooling\n",
    "- **Training Objectives**: Classification + Invariance Contrastive + Domain Adversarial losses\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. Imports and Dependencies\n",
    "2. Configuration and Reproducibility\n",
    "3. Dataset Classes\n",
    "4. Data Augmentations and Corruptions\n",
    "5. Corruption Functions for Training\n",
    "6. Model Architecture Components\n",
    "7. Tokenization and Classification Heads\n",
    "8. Loss Functions\n",
    "9. Training and Evaluation Functions\n",
    "10. Main Training Loop\n",
    "11. Model Loading\n",
    "12. FF++ Test Set Evaluation\n",
    "13. JPEG Compression Robustness Test\n",
    "14. Cross-Dataset Evaluations (DFDC, Celeb-DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842faafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Set up device for GPU acceleration if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534261e3",
   "metadata": {},
   "source": [
    "## 2. Config & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c88c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    \"\"\"Configuration class containing all hyperparameters and settings\"\"\"\n",
    "    SEED = 42\n",
    "    IMG_SIZE = 224\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_WORKERS = 0  # Set to 0 for Windows compatibility\n",
    "    LR = 1e-4\n",
    "\n",
    "    # Loss weights for multi-objective training\n",
    "    LAMBDA_INV = 0.05  # Weight for invariance contrastive loss\n",
    "    LAMBDA_DOM = 0.2   # Weight for domain adversarial loss\n",
    "\n",
    "    DATA_ROOT = \"FFPP_CViT\"  # Root directory for FaceForensics++ dataset\n",
    "    WEIGHTS_DIR = \"weights\"  # Directory to save model checkpoints\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility across all libraries\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(CFG.SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d13f5",
   "metadata": {},
   "source": [
    "## 3. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryImageFolder(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for loading binary classification image data.\n",
    "    Expects directory structure: root/real/ and root/fake/ subdirectories.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load samples from 'real' and 'fake' subdirectories\n",
    "        for label, cls in enumerate([\"real\", \"fake\"]):\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            if not os.path.exists(cls_dir):\n",
    "                continue\n",
    "            for f in os.listdir(cls_dir):\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.samples.append((os.path.join(cls_dir, f), label))\n",
    "\n",
    "        print(f\"[Dataset] Loaded {len(self.samples)} samples from {root}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b9b1c",
   "metadata": {},
   "source": [
    "## 4. Augmentations & Corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JPEGCompression:\n",
    "    \"\"\"Custom transform to simulate JPEG compression artifacts\"\"\"\n",
    "    def __init__(self, quality):\n",
    "        self.quality = quality\n",
    "\n",
    "    def __call__(self, img):\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"JPEG\", quality=self.quality)\n",
    "        buffer.seek(0)\n",
    "        return Image.open(buffer).convert(\"RGB\")\n",
    "\n",
    "\n",
    "class RandomGamma:\n",
    "    \"\"\"Random gamma correction augmentation\"\"\"\n",
    "    def __init__(self, gamma_range=(0.7, 1.5), p=0.5):\n",
    "        self.gamma_range = gamma_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            gamma = random.uniform(*self.gamma_range)\n",
    "            return transforms.functional.adjust_gamma(img, gamma)\n",
    "        return img\n",
    "\n",
    "\n",
    "# Training augmentations: aggressive transforms to improve robustness\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "    transforms.RandomAffine(2, translate=(0.02, 0.02), scale=(0.95, 1.05), shear=2),\n",
    "    transforms.ColorJitter(0.6, 0.6, 0.6, 0.15),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(3, sigma=(0.1, 2.0))\n",
    "    ], p=0.3),\n",
    "    RandomGamma(p=0.5),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAdjustSharpness(0.5)\n",
    "    ], p=0.3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Evaluation augmentations: minimal transforms for fair evaluation\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "def build_jpeg_tfms(q):\n",
    "    \"\"\"Build transforms for JPEG compression robustness testing\"\"\"\n",
    "    return transforms.Compose([\n",
    "        JPEGCompression(q),\n",
    "        transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c25f7",
   "metadata": {},
   "source": [
    "## 5. Corruption Functions (Training Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b932b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_image(x):\n",
    "    \"\"\"Apply random spatial and noise corruptions to input tensor\"\"\"\n",
    "    out = x.clone()\n",
    "\n",
    "    # Spatial degradation: downscale and upsample\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        out = F.interpolate(out, scale_factor=0.75, mode=\"bilinear\", align_corners=False)\n",
    "        out = F.interpolate(out, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # Additive noise\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        out = torch.clamp(out + 0.03 * torch.randn_like(out), 0, 1)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def freq_mix(x, alpha=0.15):\n",
    "    \"\"\"Apply frequency domain mixing corruption\"\"\"\n",
    "    fft = torch.fft.fft2(x)\n",
    "    mag, phase = torch.abs(fft), torch.angle(fft)\n",
    "\n",
    "    # Perturb magnitude with Gaussian noise\n",
    "    mag = mag * (1 + alpha * torch.randn_like(mag))\n",
    "\n",
    "    # Reconstruct signal\n",
    "    return torch.real(\n",
    "        torch.fft.ifft2(mag * torch.exp(1j * phase))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d27f91",
   "metadata": {},
   "source": [
    "## 6. Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37153573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBackbone(nn.Module):\n",
    "    \"\"\"ResNet-34 backbone for feature extraction\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = models.resnet34(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-2])  # Remove final pooling and FC\n",
    "        self.out_channels = 512\n",
    "\n",
    "        # Freeze BatchNorm layers for stability\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "\n",
    "class MultiScaleNuisanceEstimator(nn.Module):\n",
    "    \"\"\"Estimates nuisance factors using multi-scale convolutional features\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        c = channels // 4\n",
    "\n",
    "        # Multi-scale convolutions with different receptive fields\n",
    "        self.conv1 = nn.Conv2d(channels, c, 1)                    # 1x1 conv\n",
    "        self.conv3 = nn.Conv2d(channels, c, 3, padding=2, dilation=2)  # 3x3 dilated\n",
    "        self.conv5 = nn.Conv2d(channels, c, 3, padding=4, dilation=4)  # 5x5 dilated\n",
    "\n",
    "        self.proj = nn.Conv2d(3 * c, channels, 1)  # Project back to original channels\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenate multi-scale features and project\n",
    "        f = torch.cat([self.conv1(x), self.conv3(x), self.conv5(x)], dim=1)\n",
    "        return self.act(self.proj(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3eb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    \"\"\"Gradient reversal layer for domain adversarial training\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, λ):\n",
    "        ctx.λ = λ\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        return -ctx.λ * grad, None\n",
    "\n",
    "\n",
    "class NARR(nn.Module):\n",
    "    \"\"\"\n",
    "    Nuisance Aware Representation Refinement module.\n",
    "    Learns to estimate and suppress nuisance factors while preserving discriminative features.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nuisance = MultiScaleNuisanceEstimator(channels)\n",
    "\n",
    "        # Channel-wise gating: learns which channels contain nuisance\n",
    "        self.gate_c = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Spatial gating: learns which spatial locations contain nuisance\n",
    "        self.gate_s = nn.Sequential(\n",
    "            nn.Conv2d(channels, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Learnable parameters for feature refinement equation\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.3))  # Suppression strength\n",
    "        self.beta = nn.Parameter(torch.tensor(0.1))   # Enhancement strength\n",
    "\n",
    "        # Domain classifier for adversarial training\n",
    "        self.domain_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels, 2)  # Binary domain classification\n",
    "        )\n",
    "\n",
    "    def forward(self, F, lambda_grl=0.0):\n",
    "        # Estimate nuisance factors\n",
    "        N_hat = self.nuisance(F)\n",
    "\n",
    "        # Compute gating signals\n",
    "        Gc = self.gate_c(N_hat)          # [B, C, 1, 1] - Channel gates\n",
    "        Gs = self.gate_s(N_hat)          # [B, 1, H, W] - Spatial gates\n",
    "        G  = Gc * Gs                     # [B, C, H, W] - Combined gates\n",
    "\n",
    "        # Clamp learnable parameters to [0, 1]\n",
    "        alpha = torch.clamp(self.alpha, 0.0, 1.0)\n",
    "        beta  = torch.clamp(self.beta,  0.0, 1.0)\n",
    "\n",
    "        # Feature refinement equation: suppress nuisance, enhance clean features\n",
    "        F_ref = F * (1 - alpha * G + beta * (1 - G))\n",
    "\n",
    "        # Domain adversarial classification (if enabled)\n",
    "        dom = None\n",
    "        if lambda_grl > 0:\n",
    "            rev = GradReverse.apply(N_hat, lambda_grl)\n",
    "            dom = self.domain_head(rev)\n",
    "\n",
    "        return F_ref, N_hat, G, dom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac129b56",
   "metadata": {},
   "source": [
    "## 7. Tokenization & Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHead(nn.Module):\n",
    "    \"\"\"Converts feature maps to token embeddings for transformer processing\"\"\"\n",
    "    def __init__(self, in_channels, embed_dim=256, num_tokens=8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, 1)  # Project to embedding dimension\n",
    "        self.pool = nn.AdaptiveAvgPool2d((num_tokens, 1))  # Create spatial tokens\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)                  # [B, D, H, W]\n",
    "        x = self.pool(x)                  # [B, D, N, 1]\n",
    "        return x.squeeze(-1).permute(0, 2, 1)  # [B, N, D] - Token sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb57827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenClassifier(nn.Module):\n",
    "    \"\"\"Transformer-based classifier that processes token sequences\"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        # 2-layer transformer encoder\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=512,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, 2)\n",
    "        self.fc = nn.Linear(embed_dim, 1)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)        # [B, N, D] -> [B, N, D]\n",
    "        x = x.mean(dim=1)          # Token mean pooling\n",
    "        return self.fc(x).squeeze(-1)  # [B] - Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7fb8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDetector(nn.Module):\n",
    "    \"\"\"Complete NARR-based deepfake detection model\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = CNNBackbone()\n",
    "        self.narr = NARR(self.backbone.out_channels)\n",
    "        self.embedder = EmbeddingHead(self.backbone.out_channels)\n",
    "        self.classifier = TokenClassifier(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)                    # Extract features\n",
    "        f_ref, _, _, _ = self.narr(f)           # Refine features with NARR\n",
    "        tokens = self.embedder(f_ref)           # Convert to token sequence\n",
    "        return self.classifier(tokens)          # Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81198f2",
   "metadata": {},
   "source": [
    "## 8. Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a716e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard binary cross-entropy loss for classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Cross-entropy for domain adversarial training\n",
    "domain_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def invariance_contrastive_loss(z1, z2, temp=0.2):\n",
    "    \"\"\"\n",
    "    Contrastive loss to enforce invariance between clean and corrupted views.\n",
    "    Pulls representations of the same image closer, pushes different images apart.\n",
    "    \"\"\"\n",
    "    z1 = F.normalize(z1.mean(1), dim=1)  # Mean pool tokens and normalize\n",
    "    z2 = F.normalize(z2.mean(1), dim=1)\n",
    "\n",
    "    logits = (z1 @ z2.T / temp).clamp(-50, 50)  # Cosine similarity matrix\n",
    "    labels = torch.arange(z1.size(0), device=z1.device)\n",
    "    return F.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd49e71",
   "metadata": {},
   "source": [
    "## 9. Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, optimizer):\n",
    "    \"\"\"Single training epoch with multi-objective loss\"\"\"\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # ---------- CLEAN FORWARD PASS ----------\n",
    "        f = model.backbone(x)\n",
    "\n",
    "        # NARR forward with GRL enabled for domain adversarial training\n",
    "        f_ref, N_hat, _, dom_clean = model.narr(f, lambda_grl=0.1)\n",
    "        tok_n = model.embedder(N_hat)  # Tokens from nuisance features\n",
    "\n",
    "        # ---------- CORRUPTED VIEW GENERATION ----------\n",
    "        with torch.no_grad():\n",
    "            # Randomly choose between spatial/noise or frequency corruption\n",
    "            if torch.rand(1) < 0.5:\n",
    "                x_corr = corrupt_image(x)\n",
    "            else:\n",
    "                x_corr = freq_mix(x)\n",
    "\n",
    "        # Forward pass through corrupted view\n",
    "        f_c = model.backbone(x_corr)\n",
    "        _, N_hat_c, _, dom_corrupt = model.narr(f_c, lambda_grl=0.1)\n",
    "        tok_n_c = model.embedder(N_hat_c)\n",
    "\n",
    "        # ---------- INVARIANCE CONTRASTIVE LOSS ----------\n",
    "        # Enforce that nuisance representations are invariant to corruptions\n",
    "        loss_inv = invariance_contrastive_loss(tok_n, tok_n_c)\n",
    "\n",
    "        # ---------- CLASSIFICATION LOSS ----------\n",
    "        tok = model.embedder(f_ref)  # Tokens from refined features\n",
    "        logit = model.classifier(tok)\n",
    "        loss_cls = criterion(logit, y)\n",
    "\n",
    "        # ---------- DOMAIN ADVERSARIAL LOSS ----------\n",
    "        # Domain classifier tries to distinguish clean vs corrupted\n",
    "        # NARR tries to fool it by making domains indistinguishable\n",
    "        dom_y_clean = torch.zeros(x.size(0), dtype=torch.long, device=device)\n",
    "        dom_y_corrupt = torch.ones(x.size(0), dtype=torch.long, device=device)\n",
    "        loss_dom = domain_criterion(dom_clean, dom_y_clean) + domain_criterion(dom_corrupt, dom_y_corrupt)\n",
    "\n",
    "        # ---------- TOTAL LOSS ----------\n",
    "        loss = loss_cls + CFG.LAMBDA_INV * loss_inv + CFG.LAMBDA_DOM * loss_dom\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item()\n",
    "\n",
    "    return total / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12413147",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(loader, model, threshold=0.5):\n",
    "    \"\"\"Evaluate model on test set and compute comprehensive metrics\"\"\"\n",
    "    model.eval()\n",
    "    logits, labels = [], []\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        x = x.to(device)\n",
    "        logits.append(model(x).cpu())\n",
    "        labels.append(y)\n",
    "\n",
    "    logits = torch.cat(logits).numpy()\n",
    "    labels = torch.cat(labels).numpy()\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits))  # Sigmoid to get probabilities\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"auc\": roc_auc_score(labels, probs),\n",
    "        \"precision\": precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44952c81",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6506e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Create weights directory for saving checkpoints\n",
    "os.makedirs(CFG.WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize model\n",
    "model = DeepfakeDetector().to(device)\n",
    "\n",
    "# Different learning rates for different components\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": model.backbone.parameters(),  \"lr\": CFG.LR * 0.2},  # Lower LR for pretrained backbone\n",
    "    {\"params\": model.narr.parameters(),      \"lr\": CFG.LR},        # Full LR for NARR module\n",
    "    {\"params\": model.embedder.parameters(),  \"lr\": CFG.LR},        # Full LR for embedder\n",
    "    {\"params\": model.classifier.parameters(),\"lr\": CFG.LR},        # Full LR for classifier\n",
    "])\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "# Cosine annealing scheduler for gradual learning rate decay\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS\n",
    ")\n",
    "\n",
    "# Load datasets\n",
    "train_ds = BinaryImageFolder(os.path.join(CFG.DATA_ROOT, \"train\"), train_tfms)\n",
    "val_ds   = BinaryImageFolder(os.path.join(CFG.DATA_ROOT, \"val\"),   eval_tfms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Exponential moving average for AUC to reduce checkpoint saving noise\n",
    "ema_auc = None\n",
    "ema_decay = 0.8\n",
    "best_ema_auc = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e9968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 0.6926 | Val Acc: 0.7471 | AUC: 0.8872 | EMA-AUC: 0.8872 | P: 0.9646 | R: 0.7185 | F1: 0.8236\n",
      "  ✓ Saved new best model (EMA-AUC=0.8872)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Loss: 0.5595 | Val Acc: 0.8822 | AUC: 0.9352 | EMA-AUC: 0.8968 | P: 0.9491 | R: 0.9051 | F1: 0.9266\n",
      "  ✓ Saved new best model (EMA-AUC=0.8968)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 2290/2391 [17:52<00:44,  2.28it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    avg_loss = train_epoch(train_loader, model, optimizer)\n",
    "\n",
    "    # Validation phase\n",
    "    val_metrics = evaluate(val_loader, model)\n",
    "    current_auc = val_metrics[\"auc\"]\n",
    "\n",
    "    # Update exponential moving average of AUC\n",
    "    if ema_auc is None:\n",
    "        ema_auc = current_auc\n",
    "    else:\n",
    "        ema_auc = ema_decay * ema_auc + (1 - ema_decay) * current_auc\n",
    "\n",
    "    # Logging\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Loss: {avg_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_metrics['acc']:.4f} | \"\n",
    "        f\"AUC: {current_auc:.4f} | \"\n",
    "        f\"EMA-AUC: {ema_auc:.4f} | \"\n",
    "        f\"P: {val_metrics['precision']:.4f} | \"\n",
    "        f\"R: {val_metrics['recall']:.4f} | \"\n",
    "        f\"F1: {val_metrics['f1']:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save best model based on EMA-AUC\n",
    "    if ema_auc > best_ema_auc:\n",
    "        best_ema_auc = ema_auc\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f\"{CFG.WEIGHTS_DIR}/best_NARR.pt\"\n",
    "        )\n",
    "        print(f\"  ✓ Saved new best model (EMA-AUC={best_ema_auc:.4f})\")\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430501a4",
   "metadata": {},
   "source": [
    "## 11. Load Best Model (Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47897ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model...\n",
      "✓ Best model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vk200\\AppData\\Local\\Temp\\ipykernel_29720\\451708638.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f\"{CFG.WEIGHTS_DIR}/best_NARR.pt\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading best model...\")\n",
    "model.load_state_dict(\n",
    "    torch.load(f\"{CFG.WEIGHTS_DIR}/best_NARR.pt\", map_location=device)\n",
    ")\n",
    "model.eval()\n",
    "print(\"✓ Best model loaded\")\n",
    "NUM_RUNS = 3  # Number of evaluation runs for stable metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22f16f",
   "metadata": {},
   "source": [
    "## 12. FF++ Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cb7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FF++ TEST | AVERAGED OVER 3 RUNS =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ACC: 0.7737\n",
      "       AUC: 0.9182\n",
      " PRECISION: 0.9761\n",
      "    RECALL: 0.7433\n",
      "        F1: 0.8439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== FF++ TEST | AVERAGED OVER 3 RUNS =====\")\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# Load FF++ test set\n",
    "ffpp_test_ds = BinaryImageFolder(\n",
    "    os.path.join(CFG.DATA_ROOT, \"test\"),\n",
    "    eval_tfms\n",
    ")\n",
    "\n",
    "ffpp_test_loader = DataLoader(\n",
    "    ffpp_test_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Multiple evaluation runs for statistical stability\n",
    "for run_idx in range(NUM_RUNS):\n",
    "    set_seed(CFG.SEED + run_idx)\n",
    "    metrics = evaluate(ffpp_test_loader, model)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "# Average metrics across runs\n",
    "avg_metrics = {\n",
    "    k: sum(m[k] for m in all_metrics) / NUM_RUNS\n",
    "    for k in all_metrics[0]\n",
    "}\n",
    "\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k.upper():>10}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3dda53",
   "metadata": {},
   "source": [
    "## 13. JPEG Compression Robustness Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce2a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== JPEG COMPRESSION TEST | AVERAGED OVER 3 RUNS =====\n",
      "\n",
      "--- JPEG Quality 100% ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9198 |  ACC: 0.7761 |  F1: 0.8458\n",
      "\n",
      "--- JPEG Quality 90% ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9209 |  ACC: 0.8018 |  F1: 0.8664\n",
      "\n",
      "--- JPEG Quality 75% ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9013 |  ACC: 0.6977 |  F1: 0.7785\n",
      "\n",
      "--- JPEG Quality 50% ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8648 |  ACC: 0.5819 |  F1: 0.6643\n",
      "\n",
      "--- JPEG Quality 30% ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8233 |  ACC: 0.4590 |  F1: 0.5163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== JPEG COMPRESSION TEST | AVERAGED OVER 3 RUNS =====\")\n",
    "\n",
    "jpeg_qualities = [100, 90, 75, 50, 30]\n",
    "\n",
    "for q in jpeg_qualities:\n",
    "    print(f\"\\n--- JPEG Quality {q}% ---\")\n",
    "    run_metrics = []\n",
    "\n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        set_seed(CFG.SEED + run_idx)\n",
    "        # Create dataset with JPEG compression at quality q\n",
    "        jpeg_ds = BinaryImageFolder(\n",
    "            os.path.join(CFG.DATA_ROOT, \"test\"),\n",
    "            build_jpeg_tfms(q)\n",
    ")\n",
    "\n",
    "        jpeg_loader = DataLoader(\n",
    "            jpeg_ds,\n",
    "            batch_size=CFG.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "        metrics = evaluate(jpeg_loader, model)\n",
    "        run_metrics.append(metrics)\n",
    "\n",
    "    # Average metrics for this compression level\n",
    "    avg_auc = sum(m[\"auc\"] for m in run_metrics) / NUM_RUNS\n",
    "    avg_acc = sum(m[\"acc\"] for m in run_metrics) / NUM_RUNS\n",
    "    avg_f1  = sum(m[\"f1\"]  for m in run_metrics) / NUM_RUNS\n",
    "\n",
    "    print(\n",
    "        f\"AUC: {avg_auc:.4f} | \",\n",
    "        f\"ACC: {avg_acc:.4f} | \",\n",
    "        f\"F1: {avg_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712386bc",
   "metadata": {},
   "source": [
    "## 14. DFDC Cross-Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DFDC CROSS-DATASET TEST | AVERAGED OVER 3 RUNS =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ACC: 0.6606\n",
      "       AUC: 0.6278\n",
      " PRECISION: 0.8291\n",
      "    RECALL: 0.7282\n",
      "        F1: 0.7753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== DFDC CROSS-DATASET TEST | AVERAGED OVER 3 RUNS =====\")\n",
    "\n",
    "DFDC_ROOT = \"./DFDC/validation\"\n",
    "\n",
    "# Load DFDC dataset (different distribution from FF++)\n",
    "dfdc_ds = BinaryImageFolder(\n",
    "    DFDC_ROOT,\n",
    "    eval_tfms\n",
    ")\n",
    "\n",
    "dfdc_loader = DataLoader(\n",
    "    dfdc_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for run_idx in range(NUM_RUNS):\n",
    "    set_seed(CFG.SEED + run_idx)\n",
    "    metrics = evaluate(dfdc_loader, model, threshold=0.5)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    k: sum(m[k] for m in all_metrics) / NUM_RUNS\n",
    "    for k in all_metrics[0]\n",
    "}\n",
    "\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k.upper():>10}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065ff26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CELEB-DF CROSS-DATASET (NARR) | AVERAGED OVER 3 RUNS =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vk200\\AppData\\Local\\Temp\\ipykernel_29720\\2029901009.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f\"{CFG.WEIGHTS_DIR}/best_NARR.pt\", map_location=device)\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ACC: 0.6174\n",
      "       AUC: 0.6943\n",
      " PRECISION: 0.7850\n",
      "    RECALL: 0.5746\n",
      "        F1: 0.6636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== CELEB-DF CROSS-DATASET (NARR) | AVERAGED OVER 3 RUNS =====\")\n",
    "\n",
    "CELEBDF_ROOT = \"./CelebDF_images/test\"\n",
    "\n",
    "# Load Celeb-DF dataset (different distribution from FF++)\n",
    "celeb_ds = BinaryImageFolder(\n",
    "    CELEBDF_ROOT,\n",
    "    eval_tfms\n",
    ")\n",
    "\n",
    "celeb_loader = DataLoader(\n",
    "    celeb_ds,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Load best NARR model\n",
    "model.load_state_dict(\n",
    "    torch.load(f\"{CFG.WEIGHTS_DIR}/best_NARR.pt\", map_location=device)\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for run_idx in range(NUM_RUNS):\n",
    "    set_seed(CFG.SEED + run_idx)\n",
    "    metrics = evaluate(\n",
    "        celeb_loader,\n",
    "        model,\n",
    "        threshold=0.5\n",
    ")\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    k: sum(m[k] for m in all_metrics) / NUM_RUNS\n",
    "    for k in all_metrics[0]\n",
    "}\n",
    "\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k.upper():>10}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14926d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f9847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
