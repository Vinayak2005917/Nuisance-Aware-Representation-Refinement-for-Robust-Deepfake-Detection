{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24f847b",
   "metadata": {},
   "source": [
    "# Paper 8 – CLIP/ForgeLens Deepfake Detection Benchmark\n",
    "\n",
    "This notebook implements a CLIP-based deepfake detector (ForgeLens-style) built on top of an OpenCLIP visual transformer.\n",
    "\n",
    "The main stages are:\n",
    "\n",
    "- Import libraries and configure paths, image size, batch size, and optimizer settings.\n",
    "- Build an `ImageDataset` wrapper over real/fake frames (with optional JPEG compression).\n",
    "- Define the CLIP feature extractor, WSGM, FAFormer, and ForgeLens classifier.\n",
    "- Train the model on FF++ training frames and monitor the loss.\n",
    "- Save the trained model and evaluate it on an FF++ test split using standard metrics.\n",
    "\n",
    "Run the cells from top to bottom to train, save, and evaluate the model.\n",
    "\n",
    "Paper link : https://arxiv.org/pdf/2408.13697 (2408.13697v2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dddd5cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import open_clip\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    roc_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a795f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IMG_SIZE=224\n",
    "BATCH_SIZE=8\n",
    "EPOCHS=5\n",
    "LR=1e-4\n",
    "\n",
    "FFPP_REAL_PATH = r\"\"\n",
    "FFPP_FAKE_PATH = r\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a705c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self,real_path,fake_path,jpeg_quality=None):\n",
    "\n",
    "        self.samples=[]\n",
    "\n",
    "        for f in os.listdir(real_path):\n",
    "            self.samples.append((os.path.join(real_path,f),0))\n",
    "\n",
    "        for f in os.listdir(fake_path):\n",
    "            self.samples.append((os.path.join(fake_path,f),1))\n",
    "\n",
    "        self.jpeg_quality=jpeg_quality\n",
    "\n",
    "        self.tf=T.Compose([\n",
    "            T.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(\n",
    "                mean=[0.48145466,0.4578275,0.40821073],\n",
    "                std=[0.26862954,0.26130258,0.27577711]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        path,label=self.samples[idx]\n",
    "        img=Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.jpeg_quality:\n",
    "            from io import BytesIO\n",
    "            buf=BytesIO()\n",
    "            img.save(buf,\"JPEG\",quality=self.jpeg_quality)\n",
    "            img=Image.open(buf)\n",
    "\n",
    "        img=self.tf(img)\n",
    "        return img,label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847af13",
   "metadata": {},
   "source": [
    "## Dataset & Preprocessing\n",
    "\n",
    "The `ImageDataset` class wraps folders of real and fake frames and applies:\n",
    "\n",
    "- Optional JPEG compression (controlled by `jpeg_quality`).\n",
    "- Resize to `IMG_SIZE × IMG_SIZE`.\n",
    "- Normalization to the range [-1, 1] using mean and std of 0.5.\n",
    "\n",
    "It is reused for FF++ training and later for test-time evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687d7bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\open_clip\\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model,_,_ = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-16\",\n",
    "    pretrained=\"openai\"\n",
    ")\n",
    "\n",
    "clip_model = clip_model.to(DEVICE)\n",
    "\n",
    "# Freeze CLIP\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad=False\n",
    "\n",
    "clip_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413b980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPFeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self,clip_model):\n",
    "        super().__init__()\n",
    "        self.clip=clip_model\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        visual=self.clip.visual\n",
    "\n",
    "        x = visual.conv1(x)\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1).permute(0,2,1)\n",
    "\n",
    "        cls = visual.class_embedding.to(x.dtype)\n",
    "        cls = cls.unsqueeze(0).unsqueeze(0).expand(x.size(0),-1,-1)\n",
    "\n",
    "        x = torch.cat([cls,x],dim=1)\n",
    "\n",
    "        x = x + visual.positional_embedding\n",
    "        x = visual.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1,0,2)\n",
    "\n",
    "        for blk in visual.transformer.resblocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = x.permute(1,0,2)\n",
    "\n",
    "        x = visual.ln_post(x)\n",
    "\n",
    "        return x   # [B, tokens, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1184747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSGM(nn.Module):\n",
    "\n",
    "    def __init__(self,dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.guidance = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim,dim)\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self,tokens):\n",
    "\n",
    "        g = self.guidance(tokens)\n",
    "        tokens = tokens + g\n",
    "\n",
    "        return self.norm(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75263edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAFormer(nn.Module):\n",
    "\n",
    "    def __init__(self,dim,heads=8,depth=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=dim,\n",
    "                nhead=heads,\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb71d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForgeLens(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = CLIPFeatureExtractor(clip_model)\n",
    "\n",
    "        dim = clip_model.visual.transformer.width\n",
    "\n",
    "        print(\"ForgeLens token dim =\", dim)\n",
    "\n",
    "        self.wsgm = WSGM(dim)\n",
    "        self.faformer = FAFormer(dim, heads=8, depth=2)\n",
    "\n",
    "        self.cls_head = nn.Linear(dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        tokens = self.extractor(x)\n",
    "\n",
    "        tokens = self.wsgm(tokens)\n",
    "        tokens = self.faformer(tokens)\n",
    "\n",
    "        cls_token = tokens[:,0] + tokens[:,1:].mean(dim=1)\n",
    "\n",
    "        logits = self.cls_head(cls_token)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34442ad",
   "metadata": {},
   "source": [
    "## Model Architecture – ForgeLens + CLIP\n",
    "\n",
    "The overall model is composed of:\n",
    "\n",
    "- `CLIPFeatureExtractor` to obtain token embeddings from the OpenCLIP vision transformer.\n",
    "- `WSGM` (guidance module) that refines token representations with a residual MLP and LayerNorm.\n",
    "- `FAFormer`, a small Transformer encoder stack operating over tokens.\n",
    "- A classification head on the CLS token to predict real vs fake.\n",
    "\n",
    "Only the ForgeLens components are trained; the CLIP backbone is kept frozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87bb00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForgeLens token dim = 768\n"
     ]
    }
   ],
   "source": [
    "model = ForgeLens().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p:p.requires_grad,model.parameters()),\n",
    "    lr=LR\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ImageDataset(FFPP_REAL_PATH,FFPP_FAKE_PATH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cadd04",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "We now train ForgeLens on FF++ training frames using cross-entropy loss and the AdamW optimizer.\n",
    "\n",
    "The loop iterates over epochs, accumulating the average loss per epoch for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abeb90ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [14:53<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.5760109235413001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [14:25<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.5405354090447844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [14:22<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.5315555519226117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [15:32<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.5338648674360682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [15:35<00:00,  5.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.5385450400709669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "\n",
    "    for imgs,labels in tqdm(train_loader):\n",
    "\n",
    "        imgs=imgs.to(DEVICE)\n",
    "        labels=labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits=model(imgs)\n",
    "        loss=criterion(logits,labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "    print(\"Epoch\",epoch+1,\"Loss:\",total_loss/len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ac4d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model to: checkpoints\\paper8_model_BEST.pth\n"
     ]
    }
   ],
   "source": [
    "# Save trained model for Paper 8\n",
    "import os\n",
    "\n",
    "BEST_MODEL_PATH = os.path.join(\"checkpoints\", \"paper8_model_BEST.pth\")\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "print(\"Saved trained model to:\", BEST_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55fe8386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading best trained model from: checkpoints\\paper8_model_BEST.pth\n",
      "ForgeLens token dim = 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vk200\\AppData\\Local\\Temp\\ipykernel_127872\\4041966748.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Best model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Reload best model for testing\n",
    "\n",
    "print(\"\\nLoading best trained model from:\", BEST_MODEL_PATH)\n",
    "\n",
    "model = ForgeLens().to(DEVICE)\n",
    "state_dict = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "print(\"✔ Best model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f58ce",
   "metadata": {},
   "source": [
    "## Evaluation & Metrics\n",
    "\n",
    "We define a reusable `evaluate` helper that runs the model on a data loader and computes:\n",
    "\n",
    "- Accuracy\n",
    "- ROC AUC\n",
    "- Precision, Recall\n",
    "- F1 score\n",
    "\n",
    "This mirrors the evaluation used in Paper 1 so that results are comparable across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16fd94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation utilities (Paper 8)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    average_precision_score,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for imgs, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        probs = F.softmax(logits, dim=1)[:, 1]\n",
    "        preds = (probs >= 0.5).long().cpu()\n",
    "\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    probs = torch.cat(all_probs).numpy()\n",
    "    preds = torch.cat(all_preds).numpy()\n",
    "    labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    return {\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"auc\": roc_auc_score(labels, probs),\n",
    "        \"precision\": precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9fb45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FF++ TEST (Paper 8) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF++ Test Metrics: {'acc': 0.7929304110397757, 'auc': 0.5861088925143965, 'precision': 0.8285826771653543, 'recall': 0.9435975609756098, 'f1': 0.8823578735535804}\n"
     ]
    }
   ],
   "source": [
    "# FF++ TEST SET EVALUATION (Paper 8)\n",
    "\n",
    "print(\"\\n===== FF++ TEST (Paper 8) =====\")\n",
    "\n",
    "FFPP_TEST_REAL_PATH = r\"\"\n",
    "FFPP_TEST_FAKE_PATH = r\"\"\n",
    "\n",
    "ffpp_test_dataset = ImageDataset(FFPP_TEST_REAL_PATH, FFPP_TEST_FAKE_PATH)\n",
    "ffpp_test_loader = DataLoader(\n",
    "    ffpp_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    )\n",
    "\n",
    "ffpp_metrics = evaluate(ffpp_test_loader, model)\n",
    "print(\"FF++ Test Metrics:\", ffpp_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a7d69b",
   "metadata": {},
   "source": [
    "## Celeb-DF Cross-Dataset Test\n",
    "\n",
    "Finally, we test cross-dataset generalization on Celeb-DF frames,\n",
    "again reporting average detection metrics over the full evaluation split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8a433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CELEB-DF CROSS-DATASET (Paper 8) =====\n",
      "Run 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AVG: {'acc': np.float64(0.8960322071885116), 'auc': np.float64(0.5602905254982902), 'precision': np.float64(0.9004260295714643), 'recall': np.float64(0.9944185617417778), 'f1': np.float64(0.9450910764779378)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== CELEB-DF CROSS-DATASET (Paper 8) =====\")\n",
    "\n",
    "CELEB_REAL_PATH = r\"\"\n",
    "CELEB_FAKE_PATH = r\"\"\n",
    "\n",
    "celeb_dataset = ImageDataset(CELEB_REAL_PATH, CELEB_FAKE_PATH)\n",
    "celeb_loader = DataLoader(\n",
    "    celeb_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    )\n",
    "\n",
    "metrics_runs = []\n",
    "NUM_RUNS = 1\n",
    "\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"Run {run+1}/{NUM_RUNS}\")\n",
    "    metrics = evaluate(celeb_loader, model)\n",
    "    metrics_runs.append(metrics)\n",
    "\n",
    "avg = {k: np.mean([m[k] for m in metrics_runs]) for k in metrics_runs[0]}\n",
    "print(\"\\nAVG:\", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541de3a7",
   "metadata": {},
   "source": [
    "## DFDC Cross-Dataset Test\n",
    "\n",
    "We evaluate how well the model trained on FF++ generalizes to the DFDC dataset,\n",
    "using real and fake frames from a held-out DFDC split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2a0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DFDC CROSS-DATASET (Paper 8) =====\n",
      "Run 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AVG: {'acc': np.float64(0.7776416310613407), 'auc': np.float64(0.5343964246664671), 'precision': np.float64(0.7793927475980292), 'recall': np.float64(0.9968969571041911), 'f1': np.float64(0.8748283080318853)}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== DFDC CROSS-DATASET (Paper 8) =====\")\n",
    "\n",
    "DFDC_REAL_PATH = r\"\"\n",
    "DFDC_FAKE_PATH = r\"\"\n",
    "\n",
    "dfdc_dataset = ImageDataset(DFDC_REAL_PATH, DFDC_FAKE_PATH)\n",
    "dfdc_loader = DataLoader(\n",
    "    dfdc_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    )\n",
    "\n",
    "metrics_runs = []\n",
    "NUM_RUNS = 1\n",
    "\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"Run {run+1}/{NUM_RUNS}\")\n",
    "    metrics = evaluate(dfdc_loader, model)\n",
    "    metrics_runs.append(metrics)\n",
    "\n",
    "avg = {k: np.mean([m[k] for m in metrics_runs]) for k in metrics_runs[0]}\n",
    "print(\"\\nAVG:\", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344674e5",
   "metadata": {},
   "source": [
    "## JPEG Compression Test\n",
    "\n",
    "We test how robust the model is to different JPEG compression qualities on the FF++ test set.\n",
    "\n",
    "For each quality level, we recompute the metrics using `ImageDataset` with `jpeg_quality` set,\n",
    "and report the averaged results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf3c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== JPEG COMPRESSION TEST (Paper 8) =====\n",
      "\n",
      "--- JPEG Quality 100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: {'acc': np.float64(0.7931517969153568), 'auc': np.float64(0.5930055220880531), 'precision': np.float64(0.8281581636663784), 'recall': np.float64(0.9446736011477762), 'f1': np.float64(0.8825870229966908)}\n",
      "\n",
      "--- JPEG Quality 90 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: {'acc': np.float64(0.7862888347723416), 'auc': np.float64(0.5875768605462701), 'precision': np.float64(0.8290816326530612), 'recall': np.float64(0.9325681492109039), 'f1': np.float64(0.8777852802160703)}\n",
      "\n",
      "--- JPEG Quality 75 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: {'acc': np.float64(0.7989816249723267), 'auc': np.float64(0.589965282491569), 'precision': np.float64(0.8281931464174455), 'recall': np.float64(0.9535509325681492), 'f1': np.float64(0.8864621540513504)}\n",
      "\n",
      "--- JPEG Quality 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: {'acc': np.float64(0.7990554202641872), 'auc': np.float64(0.5972607548697658), 'precision': np.float64(0.8281043207473725), 'recall': np.float64(0.9538199426111909), 'f1': np.float64(0.8865274826019919)}\n",
      "\n",
      "--- JPEG Quality 30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: {'acc': np.float64(0.8017120507711608), 'auc': np.float64(0.6027977380879049), 'precision': np.float64(0.8291469010031884), 'recall': np.float64(0.9560616929698709), 'f1': np.float64(0.8880929573945275)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== JPEG COMPRESSION TEST (Paper 8) =====\")\n",
    "\n",
    "jpeg_qualities = [100, 90, 75, 50, 30]\n",
    "NUM_RUNS = 1\n",
    "\n",
    "FFPP_TEST_REAL_PATH = r\"\"\n",
    "FFPP_TEST_FAKE_PATH = r\"\"\n",
    "\n",
    "for q in jpeg_qualities:\n",
    "    print(f\"\\n--- JPEG Quality {q} ---\")\n",
    "\n",
    "    metrics_runs = []\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "        jpeg_dataset = ImageDataset(FFPP_TEST_REAL_PATH, FFPP_TEST_FAKE_PATH, jpeg_quality=q)\n",
    "        jpeg_loader = DataLoader(\n",
    "            jpeg_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        metrics = evaluate(jpeg_loader, model)\n",
    "        metrics_runs.append(metrics)\n",
    "\n",
    "    avg = {k: np.mean([m[k] for m in metrics_runs]) for k in metrics_runs[0]}\n",
    "    print(\"AVG:\", avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
