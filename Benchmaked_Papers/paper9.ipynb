{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "075f080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    roc_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1c09fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 3e-4\n",
    "\n",
    "FFPP_REAL_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\FFPP_CViT\\\\train\\\\real\"\n",
    "FFPP_FAKE_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\FFPP_CViT\\\\train\\\\fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93be72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self,real_path,fake_path,jpeg_quality=None):\n",
    "\n",
    "        self.samples=[]\n",
    "\n",
    "        for f in os.listdir(real_path):\n",
    "            self.samples.append((os.path.join(real_path,f),0))\n",
    "\n",
    "        for f in os.listdir(fake_path):\n",
    "            self.samples.append((os.path.join(fake_path,f),1))\n",
    "\n",
    "        self.jpeg_quality=jpeg_quality\n",
    "\n",
    "        self.tf=T.Compose([\n",
    "            T.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5]*3,[0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        path,label=self.samples[idx]\n",
    "        img=Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.jpeg_quality:\n",
    "            from io import BytesIO\n",
    "            buf=BytesIO()\n",
    "            img.save(buf,\"JPEG\",quality=self.jpeg_quality)\n",
    "            img=Image.open(buf)\n",
    "\n",
    "        img=self.tf(img)\n",
    "        return img,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dec3bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self,in_c,out_c,kernel=3,stride=1,padding=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_c,in_c,kernel,stride,padding,\n",
    "            groups=in_c,bias=False\n",
    "        )\n",
    "\n",
    "        self.pointwise = nn.Conv2d(in_c,out_c,1,bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.depthwise(x)\n",
    "        x=self.pointwise(x)\n",
    "        return self.bn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "710460f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self,in_c,out_c,reps,stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip = nn.Conv2d(in_c,out_c,1,stride=stride,bias=False)\n",
    "        self.skipbn = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        layers=[]\n",
    "        filters=in_c\n",
    "\n",
    "        for _ in range(reps):\n",
    "            # ✅ REMOVE inplace=True\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(SeparableConv2d(filters,out_c))\n",
    "            filters=out_c\n",
    "\n",
    "        if stride!=1:\n",
    "            layers.append(nn.MaxPool2d(3,stride,1))\n",
    "\n",
    "        self.rep = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        skip=self.skipbn(self.skip(x))\n",
    "        x=self.rep(x)\n",
    "\n",
    "        # residual add\n",
    "        return x+skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cee323eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Entry Flow\n",
    "        self.conv1 = nn.Conv2d(3,32,3,2,0,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.block1 = Block(64,128,2,2)\n",
    "        self.block2 = Block(128,256,2,2)\n",
    "        self.block3 = Block(256,728,2,2)\n",
    "\n",
    "        # Middle Flow (8 blocks)\n",
    "        self.middle = nn.Sequential(\n",
    "            *[Block(728,728,3) for _ in range(8)]\n",
    "        )\n",
    "\n",
    "        # Exit Flow\n",
    "        self.block12 = Block(728,1024,2,2)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(1024,1536)\n",
    "        self.conv4 = SeparableConv2d(1536,2048)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(2048,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x=torch.relu(self.bn1(self.conv1(x)))\n",
    "        x=torch.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "\n",
    "        x=self.middle(x)\n",
    "\n",
    "        x=self.block12(x)\n",
    "\n",
    "        x=F.relu(self.conv3(x))\n",
    "        x=F.relu(self.conv4(x))\n",
    "\n",
    "        x=self.pool(x).flatten(1)\n",
    "        logits=self.fc(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "519b15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XceptionNet().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ImageDataset(FFPP_REAL_PATH,FFPP_FAKE_PATH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ab84430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2391/2391 [14:08<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.5138833650369516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2391/2391 [13:47<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.4411662678148197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2391/2391 [13:51<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.37120652378590124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2391/2391 [13:56<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.28049582284253943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2391/2391 [14:15<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.20361332557916278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "\n",
    "    for imgs,labels in tqdm(train_loader):\n",
    "\n",
    "        imgs=imgs.to(DEVICE)\n",
    "        labels=labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits=model(imgs)\n",
    "        loss=criterion(logits,labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "    print(\"Epoch\",epoch+1,\"Loss:\",total_loss/len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fdc7c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: checkpoints\\paper9_model_BEST.pth\n"
     ]
    }
   ],
   "source": [
    "# Save final model weights\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "BEST_MODEL_PATH = os.path.join(\"checkpoints\", \"paper9_model_BEST.pth\")\n",
    "torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "print(\"Saved model to:\", BEST_MODEL_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e37160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best trained model from: checkpoints\\paper9_model_BEST.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vk200\\AppData\\Local\\Temp\\ipykernel_101008\\1054068136.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Best model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model\n",
    "BEST_MODEL_PATH = os.path.join(\"checkpoints\", \"paper9_model_BEST.pth\")\n",
    "\n",
    "print(\"Loading best trained model from:\", BEST_MODEL_PATH)\n",
    "\n",
    "# Create fresh model instance\n",
    "model = XceptionNet().to(DEVICE)\n",
    "\n",
    "# Load weights\n",
    "state_dict = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"✔ Best model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1251e485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ACC: 0.7965463803409343\n",
      "Test AUC: 0.843145278729839\n",
      "Test Precision: 0.9201281153037734\n",
      "Test Recall: 0.8243364418938307\n",
      "Test F1: 0.8696022324173486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Test on FF++ test split\n",
    "TEST_REAL_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\FFPP_CViT\\\\test\\\\real\"\n",
    "TEST_FAKE_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\FFPP_CViT\\\\test\\\\fake\"\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    ImageDataset(TEST_REAL_PATH, TEST_FAKE_PATH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "all_probs = []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        logits = model(imgs)\n",
    "        probs = F.softmax(logits, dim=1)[:, 1]\n",
    "        preds = (probs >= 0.5).long().cpu()\n",
    "\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "probs = torch.cat(all_probs).numpy()\n",
    "preds = torch.cat(all_preds).numpy()\n",
    "labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "print(\"Test ACC:\", (preds == labels).mean())\n",
    "print(\"Test AUC:\", roc_auc_score(labels, probs))\n",
    "print(\"Test Precision:\", precision_score(labels, preds, zero_division=0))\n",
    "print(\"Test Recall:\", recall_score(labels, preds, zero_division=0))\n",
    "print(\"Test F1:\", f1_score(labels, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function for comprehensive metrics\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for imgs, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        probs = F.softmax(logits, dim=1)[:,1]   # fake prob\n",
    "\n",
    "        preds = (probs >= 0.5).long().cpu()\n",
    "\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    probs = torch.cat(all_probs).numpy()\n",
    "    preds = torch.cat(all_preds).numpy()\n",
    "    labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    return {\n",
    "        \"acc\": (preds == labels).mean(),\n",
    "        \"auc\": roc_auc_score(labels, probs),\n",
    "        \"precision\": precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, zero_division=0),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== JPEG COMPRESSION TEST (Paper9) =====\n",
      "\n",
      "--- JPEG Quality 100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.7896 | AUC: 0.8426 | Precision: 0.9225 | Recall: 0.8126 | F1: 0.8641\n",
      "\n",
      "--- JPEG Quality 90 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.7614 | AUC: 0.8382 | Precision: 0.9295 | Recall: 0.7684 | F1: 0.8413\n",
      "\n",
      "--- JPEG Quality 75 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.7622 | AUC: 0.8298 | Precision: 0.9280 | Recall: 0.7708 | F1: 0.8421\n",
      "\n",
      "--- JPEG Quality 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.7100 | AUC: 0.8117 | Precision: 0.9327 | Recall: 0.6980 | F1: 0.7984\n",
      "\n",
      "--- JPEG Quality 30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.6343 | AUC: 0.7855 | Precision: 0.9361 | Recall: 0.5964 | F1: 0.7286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# JPEG Compression Testing\n",
    "import io\n",
    "\n",
    "class JPEGCompression:\n",
    "    def __init__(self, quality):\n",
    "        self.quality = quality\n",
    "\n",
    "    def __call__(self, img_tensor):\n",
    "        # UNNORMALIZE\n",
    "        img = img_tensor.clone()\n",
    "        img = img * 0.5 + 0.5     # [-1,1] -> [0,1]\n",
    "        img = img.clamp(0,1)\n",
    "\n",
    "        img = img.permute(1,2,0).cpu().numpy()\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "        pil_img = Image.fromarray(img)\n",
    "        buffer = io.BytesIO()\n",
    "        pil_img.save(buffer, format=\"JPEG\", quality=self.quality)\n",
    "        buffer.seek(0)\n",
    "\n",
    "        comp = Image.open(buffer).convert(\"RGB\")\n",
    "        comp = np.array(comp) / 255.0\n",
    "        comp = torch.tensor(comp).permute(2,0,1).float()\n",
    "\n",
    "        # RENORMALIZE\n",
    "        comp = (comp - 0.5) / 0.5\n",
    "\n",
    "        return comp\n",
    "\n",
    "print(\"\\n===== JPEG COMPRESSION TEST (Paper9) =====\")\n",
    "\n",
    "jpeg_qualities = [100, 90, 75, 50, 30]\n",
    "\n",
    "for q in jpeg_qualities:\n",
    "    print(f\"\\n--- JPEG Quality {q} ---\")\n",
    "\n",
    "    class JPEGWrapper(torch.utils.data.Dataset):\n",
    "        def __init__(self, base_dataset, quality):\n",
    "            self.base = base_dataset\n",
    "            self.comp = JPEGCompression(quality)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.base)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img, label = self.base[idx]\n",
    "            img = self.comp(img)\n",
    "            return img, label\n",
    "\n",
    "    jpeg_dataset = JPEGWrapper(\n",
    "        ImageDataset(TEST_REAL_PATH, TEST_FAKE_PATH), q\n",
    "    )\n",
    "    jpeg_loader = DataLoader(\n",
    "        jpeg_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    metrics = evaluate(jpeg_loader, model)\n",
    "    print(f\"ACC: {metrics['acc']:.4f} | AUC: {metrics['auc']:.4f} | \"\n",
    "          f\"Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6735c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DFDC CROSS-DATASET (Paper9) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.7600\n",
      "AUC: 0.4198\n",
      "Precision: 0.7770\n",
      "Recall: 0.9707\n",
      "F1: 0.8631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# DFDC Cross-Dataset Test\n",
    "DFDC_REAL_PATH = r\"C:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\DFDC\\train\\real\"\n",
    "DFDC_FAKE_PATH = r\"C:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\DFDC\\train\\fake\"\n",
    "\n",
    "print(\"\\n===== DFDC CROSS-DATASET (Paper9) =====\")\n",
    "\n",
    "dfdc_dataset = ImageDataset(DFDC_REAL_PATH, DFDC_FAKE_PATH)\n",
    "dfdc_loader = DataLoader(\n",
    "    dfdc_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "metrics = evaluate(dfdc_loader, model)\n",
    "print(f\"ACC: {metrics['acc']:.4f}\")\n",
    "print(f\"AUC: {metrics['auc']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1: {metrics['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7130638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CELEB-DF CROSS-DATASET (Paper9) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.8239\n",
      "AUC: 0.7013\n",
      "Precision: 0.9256\n",
      "Recall: 0.8747\n",
      "F1: 0.8994\n"
     ]
    }
   ],
   "source": [
    "# Celeb-DF Cross-Dataset Test\n",
    "CELEB_REAL_PATH = r\"C:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\CelebDF_images\\train\\real\"\n",
    "CELEB_FAKE_PATH = r\"C:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\CelebDF_images\\train\\fake\"\n",
    "\n",
    "print(\"\\n===== CELEB-DF CROSS-DATASET (Paper9) =====\")\n",
    "\n",
    "celeb_dataset = ImageDataset(CELEB_REAL_PATH, CELEB_FAKE_PATH)\n",
    "celeb_loader = DataLoader(\n",
    "    celeb_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "metrics = evaluate(celeb_loader, model)\n",
    "print(f\"ACC: {metrics['acc']:.4f}\")\n",
    "print(f\"AUC: {metrics['auc']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1: {metrics['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5f14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
