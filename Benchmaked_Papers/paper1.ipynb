{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e89c6f",
   "metadata": {},
   "source": [
    "# Paper 1 – RFFR Deepfake Detection Benchmark\n",
    "\n",
    "This notebook implements and evaluates the **Residual Feature Fusion for Robust Deepfake Detection (RFFR)**-style model on multiple datasets:\n",
    "\n",
    "- **FF++ (CViT frames)** for in-dataset training and testing\n",
    "\n",
    "- **DFDC** and **Celeb-DF** for cross-dataset generalization\n",
    "\n",
    "- **JPEG compression robustness** experiments to test stability under varying quality levels\n",
    "\n",
    "> Run the notebook top-to-bottom to train (or load) the model, evaluate it on different benchmarks, and finally see summary visualizations of the main metrics.\n",
    "\n",
    "Paper link : https://arxiv.org/pdf/2303.08439 (2303.08439v1.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31be3090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8        # safe for 3050\n",
    "EPOCHS = 0\n",
    "LR = 2e-5\n",
    "\n",
    "FFPP_REAL_PATH = r\"\"\n",
    "FFPP_FAKE_PATH = r\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68f57c",
   "metadata": {},
   "source": [
    "## 2. Dataset & Preprocessing\n",
    "\n",
    "\n",
    "In this section we define the **FFPPDataset** wrapper around image folders of real and fake faces.\n",
    "\n",
    "\n",
    "Key points:\n",
    "\n",
    "\n",
    "- Images are read from disk using `PIL.Image`.\n",
    "- Each image is resized to `IMG_SIZE × IMG_SIZE`.\n",
    "- Images are normalized to the `[-1, 1]` range using mean/std of 0.5.\n",
    "\n",
    "\n",
    "> The resulting dataset is used for FF++, DFDC, and Celeb-DF evaluations by simply changing the input folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357a2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFPPDataset(Dataset):\n",
    "    def __init__(self, real_path, fake_path):\n",
    "        self.samples = []\n",
    "\n",
    "        for f in os.listdir(real_path):\n",
    "            self.samples.append((os.path.join(real_path, f), 0))\n",
    "\n",
    "        for f in os.listdir(fake_path):\n",
    "            self.samples.append((os.path.join(fake_path, f), 1))\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bc82e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class FFPPDataset(Dataset):\n",
    "    def __init__(self, real_path, fake_path):\n",
    "        self.samples = []\n",
    "\n",
    "        for f in os.listdir(real_path):\n",
    "            self.samples.append((os.path.join(real_path, f), 0))\n",
    "\n",
    "        for f in os.listdir(fake_path):\n",
    "            self.samples.append((os.path.join(fake_path, f), 1))\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "486d4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockMasker:\n",
    "    def __init__(self, k=4):\n",
    "        self.k = k\n",
    "\n",
    "    def mask(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        bh, bw = H // self.k, W // self.k\n",
    "        out = x.clone()\n",
    "\n",
    "        for b in range(B):\n",
    "            i = random.randint(0, self.k - 1)\n",
    "            j = random.randint(0, self.k - 1)\n",
    "\n",
    "            out[b, :, i*bh:(i+1)*bh, j*bw:(j+1)*bw] = 0\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb37b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class ResidualGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inpainter = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 3, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, original, masked):\n",
    "        recon = self.inpainter(masked)\n",
    "        residual = 4.0 * (recon - original)\n",
    "        return residual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b469e",
   "metadata": {},
   "source": [
    "## 3. Model Architecture – RFFR\n",
    "\n",
    "\n",
    "The model follows a **dual-branch RFFR-style architecture**:\n",
    "\n",
    "\n",
    "1. **BlockMasker** randomly masks out a spatial block of the input face.\n",
    "2. **ResidualGenerator** inpaints the masked image and computes a residual map \\(4 \\times (\\hat{x} - x)\\).\n",
    "3. **DualBranchClassifier** extracts features from both the original image and the residual using a shared ViT backbone and fuses them for classification.\n",
    "\n",
    "\n",
    "> This design encourages the model to focus on subtle manipulation artifacts rather than only global appearance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e94f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class DualBranchClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            \"vit_base_patch16_224\",\n",
    "            pretrained=True,\n",
    "            num_classes=0\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(768 * 2, 2)\n",
    "\n",
    "    def forward(self, img, residual):\n",
    "        f_img = self.backbone(img)\n",
    "        f_res = self.backbone(residual)\n",
    "\n",
    "        fused = torch.cat([f_img, f_res], dim=1)\n",
    "        return self.fc(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9bc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class RFFRModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.masker = BlockMasker(k=4)\n",
    "        self.residual_gen = ResidualGenerator()\n",
    "        self.classifier = DualBranchClassifier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        masked = self.masker.mask(x)\n",
    "        residual = self.residual_gen(x, masked)\n",
    "        return self.classifier(x, residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d2f70de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class RFFRModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.masker = BlockMasker(k=4)\n",
    "        self.residual_gen = ResidualGenerator()\n",
    "        self.classifier = DualBranchClassifier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        masked = self.masker.mask(x)\n",
    "        residual = self.residual_gen(x, masked)\n",
    "        return self.classifier(x, residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a7a2f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 30602 Val: 7650\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "dataset = FFPPDataset(FFPP_REAL_PATH, FFPP_FAKE_PATH)\n",
    "\n",
    "val_ratio = 0.2\n",
    "val_size = int(len(dataset) * val_ratio)\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(train_dataset), \"Val:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01546aed",
   "metadata": {},
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "\n",
    "Here we:\n",
    "\n",
    "\n",
    "- Instantiate the `FFPPDataset` on FF++ training frames.\n",
    "- Split the data into **train** and **validation** subsets.\n",
    "- Wrap them in PyTorch **DataLoader** objects with shuffling for training.\n",
    "\n",
    "\n",
    "> Adjust `BATCH_SIZE`, `EPOCHS`, and paths above to match your GPU memory and dataset locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7016ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "model = RFFRModel().to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dead1259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def run_epoch(loader, model, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for imgs, labels in tqdm(loader, leave=False):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(1)\n",
    "        total_loss += loss.item()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89d28804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "best_val = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_acc = run_epoch(train_loader, model, optimizer)\n",
    "    val_loss, val_acc = run_epoch(val_loader, model)\n",
    "\n",
    "    print(f\"Train | loss: {train_loss:.4f} acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   | loss: {val_loss:.4f} acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save(model.state_dict(), \"best_rffr.pth\")\n",
    "        print(\"✔ Saved best model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da128ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading best trained model from: best_rffr.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vk200\\AppData\\Local\\Temp\\ipykernel_96284\\4054472400.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Best model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# %% =========================\n",
    "# LOAD BEST MODEL FOR TESTING\n",
    "# =========================\n",
    "\n",
    "BEST_MODEL_PATH = \"best_rffr.pth\"\n",
    "\n",
    "print(\"\\nLoading best trained model from:\", BEST_MODEL_PATH)\n",
    "\n",
    "# Create fresh model instance\n",
    "model = RFFRModel().to(DEVICE)\n",
    "\n",
    "# Load weights\n",
    "state_dict = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"✔ Best model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d5bcc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Val Accuracy: 0.9887581699346405\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        preds = model(imgs).argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(\"Final Val Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "209be391",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(loader, model):\n\u001b[32m     19\u001b[39m     model.eval()\n\u001b[32m     21\u001b[39m     all_probs = []\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# %% =========================\n",
    "# Evaluation Utilities (Paper1)\n",
    "# =============================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for imgs, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "\n",
    "        logits = model(imgs)                  # (B,2)\n",
    "        probs = F.softmax(logits, dim=1)[:,1]   # fake prob\n",
    "\n",
    "        preds = (probs >= 0.5).long().cpu()\n",
    "\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    probs = torch.cat(all_probs).numpy()\n",
    "    preds = torch.cat(all_preds).numpy()\n",
    "    labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    return {\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"auc\": roc_auc_score(labels, probs),\n",
    "        \"precision\": precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, zero_division=0),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be398423",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Metrics\n",
    "\n",
    "\n",
    "We define a reusable `evaluate` helper that computes the main detection metrics:\n",
    "\n",
    "\n",
    "- **Accuracy (ACC)**\n",
    "- **Area under ROC curve (AUC)**\n",
    "- **Precision / Recall**\n",
    "- **F1 score**\n",
    "\n",
    "\n",
    "> These metrics are used for FF++ test, JPEG compression robustness, and cross-dataset (DFDC, Celeb-DF) experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FF++ TEST (Paper1) | 3-RUN AVG =====\n",
      "\n",
      "Run 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.8673898605268984, 'auc': 0.930978795863652, 'precision': 0.9507564806784234, 'recall': 0.8846843615494978, 'f1': 0.9165311904872497}\n",
      "\n",
      "AVG: {'acc': np.float64(0.8673898605268984), 'auc': np.float64(0.930978795863652), 'precision': np.float64(0.9507564806784234), 'recall': np.float64(0.8846843615494978), 'f1': np.float64(0.9165311904872497)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# %% =========================\n",
    "# FF++ TEST SET | 3-RUN AVG\n",
    "# ============================\n",
    "\n",
    "print(\"\\n===== FF++ TEST (Paper1) | 3-RUN AVG =====\")\n",
    "\n",
    "FFPP_REAL_PATH = r\"\"\n",
    "FFPP_FAKE_PATH = r\"\"\n",
    "\n",
    "NUM_RUNS = 1\n",
    "all_metrics = []\n",
    "\n",
    "ffpp_test_dataset = FFPPDataset(FFPP_REAL_PATH, FFPP_FAKE_PATH)\n",
    "ffpp_test_loader = DataLoader(\n",
    "    ffpp_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"\\nRun {run+1}/{NUM_RUNS}\")\n",
    "\n",
    "    metrics = evaluate(ffpp_test_loader, model)\n",
    "    all_metrics.append(metrics)\n",
    "    print(metrics)\n",
    "\n",
    "# Average\n",
    "avg = {k: np.mean([m[k] for m in all_metrics]) for k in all_metrics[0]}\n",
    "print(\"\\nAVG:\", avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc57d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== JPEG COMPRESSION TEST (Paper1) | 3-RUN AVG =====\n",
      "\n",
      "--- JPEG Quality 100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/1694 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: {'acc': np.float64(0.866135340565272), 'auc': np.float64(0.9300870109377233), 'precision': np.float64(0.9533009708737864), 'recall': np.float64(0.8804698708751794), 'f1': np.float64(0.9154391198955808)}\n",
      "\n",
      "--- JPEG Quality 90 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: {'acc': np.float64(0.8589034019629548), 'auc': np.float64(0.9298905517483075), 'precision': np.float64(0.9602510460251046), 'recall': np.float64(0.864329268292683), 'f1': np.float64(0.9097687588485135)}\n",
      "\n",
      "--- JPEG Quality 75 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: {'acc': np.float64(0.8479816987676186), 'auc': np.float64(0.9181844098419774), 'precision': np.float64(0.9525184152896675), 'recall': np.float64(0.8580523672883787), 'f1': np.float64(0.9028210208510237)}\n",
      "\n",
      "--- JPEG Quality 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: {'acc': np.float64(0.8047376577374363), 'auc': np.float64(0.9022546196316853), 'precision': np.float64(0.9588908070781182), 'recall': np.float64(0.7968974175035868), 'f1': np.float64(0.8704211557296768)}\n",
      "\n",
      "--- JPEG Quality 30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG: {'acc': np.float64(0.7569921039037709), 'auc': np.float64(0.8818722029982602), 'precision': np.float64(0.9643152546378353), 'recall': np.float64(0.731796987087518), 'f1': np.float64(0.8321182768289574)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# %% =========================\n",
    "# JPEG COMPRESSION TEST\n",
    "# =========================\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class JPEGCompression:\n",
    "    def __init__(self, quality):\n",
    "        self.quality = quality\n",
    "\n",
    "    def __call__(self, img_tensor):\n",
    "\n",
    "        # UNNORMALIZE\n",
    "        img = img_tensor.clone()\n",
    "        img = img * 0.5 + 0.5     # [-1,1] -> [0,1]\n",
    "        img = img.clamp(0,1)\n",
    "\n",
    "        img = img.permute(1,2,0).cpu().numpy()\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "        pil_img = Image.fromarray(img)\n",
    "        buffer = io.BytesIO()\n",
    "        pil_img.save(buffer, format=\"JPEG\", quality=self.quality)\n",
    "        buffer.seek(0)\n",
    "\n",
    "        comp = Image.open(buffer).convert(\"RGB\")\n",
    "        comp = np.array(comp) / 255.0\n",
    "        comp = torch.tensor(comp).permute(2,0,1).float()\n",
    "\n",
    "        # RENORMALIZE\n",
    "        comp = (comp - 0.5) / 0.5\n",
    "\n",
    "        return comp\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n===== JPEG COMPRESSION TEST (Paper1) | 3-RUN AVG =====\")\n",
    "\n",
    "jpeg_qualities = [100, 90, 75, 50, 30]\n",
    "\n",
    "for q in jpeg_qualities:\n",
    "    print(f\"\\n--- JPEG Quality {q} ---\")\n",
    "\n",
    "    class JPEGWrapper(torch.utils.data.Dataset):\n",
    "        def __init__(self, base_dataset, quality):\n",
    "            self.base = base_dataset\n",
    "            self.comp = JPEGCompression(quality)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.base)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img, label = self.base[idx]\n",
    "            img = self.comp(img)\n",
    "            return img, label\n",
    "\n",
    "    metrics_runs = []\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "\n",
    "        jpeg_dataset = JPEGWrapper(ffpp_test_dataset, q)\n",
    "        jpeg_loader = DataLoader(\n",
    "            jpeg_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        metrics = evaluate(jpeg_loader, model)\n",
    "        metrics_runs.append(metrics)\n",
    "\n",
    "    avg = {k: np.mean([m[k] for m in metrics_runs]) for k in metrics_runs[0]}\n",
    "    print(\"AVG:\", avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cb607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DFDC CROSS-DATASET (Paper1) | 3-RUN AVG =====\n",
      "Run 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AVG: {'acc': np.float64(0.7222571468146995), 'auc': np.float64(0.6375245349462827), 'precision': np.float64(0.8043250649535307), 'recall': np.float64(0.8506028378489214), 'f1': np.float64(0.826816904402825)}\n"
     ]
    }
   ],
   "source": [
    "# %% =========================\n",
    "# DFDC CROSS DATASET TEST\n",
    "# ============================\n",
    "\n",
    "DFDC_REAL_PATH = r\"\"\n",
    "DFDC_FAKE_PATH = r\"\"\n",
    "print(\"\\n===== DFDC CROSS-DATASET (Paper1) | 3-RUN AVG =====\")\n",
    "\n",
    "dfdc_dataset = FFPPDataset(DFDC_REAL_PATH, DFDC_FAKE_PATH)\n",
    "dfdc_loader = DataLoader(\n",
    "    dfdc_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "metrics_runs = []\n",
    "\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"Run {run+1}/{NUM_RUNS}\")\n",
    "    metrics = evaluate(dfdc_loader, model)\n",
    "    metrics_runs.append(metrics)\n",
    "\n",
    "avg = {k: np.mean([m[k] for m in metrics_runs]) for k in metrics_runs[0]}\n",
    "print(\"\\nAVG:\", avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67ac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CELEB-DF CROSS-DATASET (Paper1) | 3-RUN AVG =====\n",
      "Run 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AVG: {'acc': np.float64(0.8357267369469578), 'auc': np.float64(0.7835831147047972), 'precision': np.float64(0.9361126150514347), 'recall': np.float64(0.8773006134969326), 'f1': np.float64(0.9057529288503667)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# %% =========================\n",
    "# CELEB-DF CROSS DATASET TEST\n",
    "# =========================\n",
    "\n",
    "CELEB_REAL_PATH = r\"\"\n",
    "CELEB_FAKE_PATH = r\"\"\n",
    "\n",
    "print(\"\\n===== CELEB-DF CROSS-DATASET (Paper1) | 3-RUN AVG =====\")\n",
    "\n",
    "celeb_dataset = FFPPDataset(CELEB_REAL_PATH, CELEB_FAKE_PATH)\n",
    "celeb_loader = DataLoader(\n",
    "    celeb_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "metrics_runs = []\n",
    "\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"Run {run+1}/{NUM_RUNS}\")\n",
    "    metrics = evaluate(celeb_loader, model)\n",
    "    metrics_runs.append(metrics)\n",
    "\n",
    "avg = {k: np.mean([m[k] for m in metrics_runs]) for k in metrics_runs[0]}\n",
    "print(\"\\nAVG:\", avg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
