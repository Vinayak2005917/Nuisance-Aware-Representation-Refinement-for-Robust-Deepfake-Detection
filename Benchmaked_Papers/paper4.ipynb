{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c547c16",
   "metadata": {},
   "source": [
    "# Paper 4 – Deepfake Detection Benchmark\n",
    "\n",
    "\n",
    "This notebook implements the **Paper 4** variant of the deepfake detector.\n",
    "\n",
    "\n",
    "\n",
    "The code cells below typically follow this structure:\n",
    "\n",
    "- Import libraries and configure paths/devices.\n",
    "- Prepare datasets and data loaders for the relevant benchmarks.\n",
    "- Define the model architecture and loss functions used in Paper 4.\n",
    "- Train and evaluate the model, printing metrics for comparison.\n",
    "\n",
    "\n",
    "\n",
    "> Run the cells from top to bottom to reproduce the results reported for Paper 4.\n",
    "\n",
    "Paper link : https://arxiv.org/pdf/2505.20653 (2505.20653v1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2075b141",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\__init__.py:2486\u001b[39m\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[32m   2485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[32m-> \u001b[39m\u001b[32m2486\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[32m   2488\u001b[39m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[32m   2489\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mTORCH_CUDA_SANITIZER\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\_meta_registrations.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     _add_op_to_registry,\n\u001b[32m     12\u001b[39m     _convert_out_params,\n\u001b[32m     13\u001b[39m     global_decomposition_table,\n\u001b[32m     14\u001b[39m     meta_table,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\_decomp\\__init__.py:249\u001b[39m\n\u001b[32m    245\u001b[39m             decompositions.pop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecompositions\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_refs\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# See NOTE [Core ATen Ops]\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\_decomp\\decompositions.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_meta_registrations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprims\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\_prims\\__init__.py:1146\u001b[39m\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.minimum(a, b)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1139\u001b[39m minimum = _make_elementwise_binary_prim(\n\u001b[32m   1140\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mminimum\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1141\u001b[39m     impl_aten=_minimum_aten,\n\u001b[32m   1142\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1143\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m   1144\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1146\u001b[39m mul = \u001b[43m_make_elementwise_binary_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmul\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimpl_aten\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEFAULT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m ne = _make_elementwise_binary_prim(\n\u001b[32m   1154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mne\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1155\u001b[39m     impl_aten=torch.ne,\n\u001b[32m   1156\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1157\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.ALWAYS_BOOL,\n\u001b[32m   1158\u001b[39m )\n\u001b[32m   1160\u001b[39m nextafter = _make_elementwise_binary_prim(\n\u001b[32m   1161\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnextafter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1162\u001b[39m     impl_aten=torch.nextafter,\n\u001b[32m   1163\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1164\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m   1165\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\_prims\\__init__.py:506\u001b[39m, in \u001b[36m_make_elementwise_binary_prim\u001b[39m\u001b[34m(name, type_promotion, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_elementwise_binary_prim\u001b[39m(\n\u001b[32m    500\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m, *, type_promotion: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, **kwargs\n\u001b[32m    501\u001b[39m ):\n\u001b[32m    502\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    503\u001b[39m \u001b[33;03m    Creates an elementwise binary prim.\u001b[39;00m\n\u001b[32m    504\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m(Tensor self, Tensor other) -> Tensor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_prim_elementwise_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRETURN_TYPE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNEW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\_prims\\__init__.py:319\u001b[39m, in \u001b[36m_make_prim\u001b[39m\u001b[34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api, register_conj_neg_fallthrough)\u001b[39m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg.alias_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m arg.alias_info.is_write:\n\u001b[32m    318\u001b[39m         mutates_args.append(arg.name)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m prim_def = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprims::\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_prim_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m prim_def.register_fake(meta)\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# all view ops get conj/neg fallthroughs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:157\u001b[39m, in \u001b[36mcustom_op\u001b[39m\u001b[34m(name, fn, mutates_args, device_types, schema)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:138\u001b[39m, in \u001b[36mcustom_op.<locals>.inner\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m    135\u001b[39m     schema_str = schema\n\u001b[32m    137\u001b[39m namespace, opname = name.split(\u001b[33m\"\u001b[39m\u001b[33m::\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m result = \u001b[43mCustomOpDef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Check that schema's alias annotations match those of `mutates_args`.\u001b[39;00m\n\u001b[32m    141\u001b[39m     expected = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:185\u001b[39m, in \u001b[36mCustomOpDef.__init__\u001b[39m\u001b[34m(self, namespace, name, schema, fn)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m._torch_dispatch_fns: Dict[\u001b[38;5;28mtype\u001b[39m, Callable] = {}\n\u001b[32m    183\u001b[39m \u001b[38;5;28mself\u001b[39m._vmap_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28mself\u001b[39m._lib = \u001b[43mget_library_allowing_overwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_namespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28mself\u001b[39m._register_to_dispatcher()\n\u001b[32m    187\u001b[39m \u001b[38;5;28mself\u001b[39m._disabled_kernel: Set = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:805\u001b[39m, in \u001b[36mget_library_allowing_overwrite\u001b[39m\u001b[34m(namespace, name)\u001b[39m\n\u001b[32m    802\u001b[39m     OPDEF_TO_LIB[qualname]._destroy()\n\u001b[32m    803\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m OPDEF_TO_LIB[qualname]\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m lib = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFRAGMENT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: TOR901\u001b[39;00m\n\u001b[32m    806\u001b[39m OPDEF_TO_LIB[qualname] = lib\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m lib\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\library.py:85\u001b[39m, in \u001b[36mLibrary.__init__\u001b[39m\u001b[34m(self, ns, kind, dispatch_key)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;129;01min\u001b[39;00m _reserved_namespaces \u001b[38;5;129;01mand\u001b[39;00m (kind == \u001b[33m\"\u001b[39m\u001b[33mDEF\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m kind == \u001b[33m\"\u001b[39m\u001b[33mFRAGMENT\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     81\u001b[39m         ns,\n\u001b[32m     82\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is a reserved namespace. Please try creating a library with another name.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     83\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m frame = \u001b[43mtraceback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     86\u001b[39m filename, lineno = frame.filename, frame.lineno\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.m: Optional[Any] = torch._C._dispatch_library(\n\u001b[32m     88\u001b[39m     kind, ns, dispatch_key, filename, lineno\n\u001b[32m     89\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:228\u001b[39m, in \u001b[36mextract_stack\u001b[39m\u001b[34m(f, limit)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    227\u001b[39m     f = sys._getframe().f_back\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m stack = \u001b[43mStackSummary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m stack.reverse()\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:390\u001b[39m, in \u001b[36mStackSummary.extract\u001b[39m\u001b[34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[39m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[32m    388\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:429\u001b[39m, in \u001b[36mStackSummary._extract_from_extended_frame_gen\u001b[39m\u001b[34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[39m\n\u001b[32m    425\u001b[39m     result.append(FrameSummary(\n\u001b[32m    426\u001b[39m         filename, lineno, name, lookup_line=\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m=f_locals,\n\u001b[32m    427\u001b[39m         end_lineno=end_lineno, colno=colno, end_colno=end_colno))\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[43mlinecache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\linecache.py:72\u001b[39m, in \u001b[36mcheckcache\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     stat = os.stat(fullname)\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m     74\u001b[39m     cache.pop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd857c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 0.005\n",
    "\n",
    "# RoGA params from paper\n",
    "RHO = 0.1\n",
    "ALPHA = 0.0002\n",
    "\n",
    "FFPP_REAL_PATH = r\"C:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\FFPP_CViT\\train\\real\"\n",
    "FFPP_FAKE_PATH = r\"C:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\FFPP_CViT\\train\\fake\"\n",
    "\n",
    "# optional cross dataset\n",
    "CELEBDF_PATH = \"PATH_TO_CELEBDF\"\n",
    "DFDC_PATH = \"PATH_TO_DFDC\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, real_path, fake_path=None, jpeg_quality=None):\n",
    "\n",
    "        self.samples = []\n",
    "\n",
    "        if fake_path is not None:\n",
    "            for f in os.listdir(real_path):\n",
    "                self.samples.append((os.path.join(real_path,f),0))\n",
    "            for f in os.listdir(fake_path):\n",
    "                self.samples.append((os.path.join(fake_path,f),1))\n",
    "        else:\n",
    "            for f in os.listdir(real_path):\n",
    "                self.samples.append((os.path.join(real_path,f),0))\n",
    "\n",
    "        self.jpeg_quality = jpeg_quality\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5]*3,[0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        path,label=self.samples[idx]\n",
    "\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.jpeg_quality is not None:\n",
    "            from io import BytesIO\n",
    "            buffer = BytesIO()\n",
    "            img.save(buffer, format=\"JPEG\", quality=self.jpeg_quality)\n",
    "            img = Image.open(buffer)\n",
    "\n",
    "        img = self.transform(img)\n",
    "        return img,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features,2)\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b91357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roga_step(model, imgs, labels, optimizer, criterion):\n",
    "\n",
    "    # ===== Forward 1 (ERM gradient) =====\n",
    "    logits = model(imgs)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(create_graph=True)\n",
    "\n",
    "    # compute gradient norm\n",
    "    grad_norm = torch.norm(\n",
    "        torch.stack([\n",
    "            p.grad.norm()\n",
    "            for p in model.parameters()\n",
    "            if p.grad is not None\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # save original weights\n",
    "    original_params = []\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            original_params.append(None)\n",
    "            continue\n",
    "\n",
    "        eps = RHO * p.grad / (grad_norm + 1e-12)\n",
    "        original_params.append(p.data.clone())\n",
    "        p.data = p.data + eps\n",
    "\n",
    "    # ===== Forward 2 (perturbed) =====\n",
    "    logits_pert = model(imgs)\n",
    "    loss_pert = criterion(logits_pert, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_pert.backward(create_graph=True)\n",
    "\n",
    "    # ===== Gradient Alignment Term =====\n",
    "    align = 0.0\n",
    "    for p, orig in zip(model.parameters(), original_params):\n",
    "        if p.grad is None or orig is None:\n",
    "            continue\n",
    "        align += (p.grad * (p.data - orig)).sum()\n",
    "\n",
    "    roga_loss = loss_pert - ALPHA * align\n",
    "\n",
    "    # restore original weights BEFORE final backward\n",
    "    for p, orig in zip(model.parameters(), original_params):\n",
    "        if orig is not None:\n",
    "            p.data = orig\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    roga_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return roga_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ImageDataset(FFPP_REAL_PATH,FFPP_FAKE_PATH)\n",
    "train_loader = DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6079c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4782 [00:00<?, ?it/s]c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\engine.cpp:1206.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 4782/4782 [36:58<00:00,  2.16it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7571298806413553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [35:22<00:00,  2.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.5813964470342232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [34:57<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.5549187639775709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [35:31<00:00,  2.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.5411997244933667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4782/4782 [36:30<00:00,  2.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.5307824743661558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs,labels in tqdm(train_loader):\n",
    "\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        loss = roga_step(model,imgs,labels,optimizer,criterion)\n",
    "        total_loss += loss\n",
    "\n",
    "    print(\"Epoch\",epoch+1,\"Loss:\",total_loss/len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"./checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"paper4_model\"\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss):\n",
    "    path = os.path.join(SAVE_DIR, f\"{MODEL_NAME}_BEST.pth\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss\n",
    "    }, path)\n",
    "    print(\"Saved BEST checkpoint:\", path)\n",
    "\n",
    "# Save checkpoint after training (uses last epoch's stats)\n",
    "save_checkpoint(model, optimizer, epoch+1, total_loss/len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(loader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # show progress so long runs (e.g., cross-dataset) are visible\n",
    "    for imgs, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        logits = model(imgs)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)[:,1]\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        all_probs.extend(probs.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    # ===== Metrics =====\n",
    "    acc = correct / total\n",
    "\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    ap  = average_precision_score(all_labels, all_probs)\n",
    "\n",
    "    # ===== EER =====\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    fnr = 1 - tpr\n",
    "    eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    return {\n",
    "        \"ACC\": acc,\n",
    "        \"AUC\": auc,\n",
    "        \"AP\": ap,\n",
    "        \"EER\": eer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d4733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FF++ Evaluation =====\n",
      "FF++ Metrics:\n",
      "{'ACC': 0.7458694970197637, 'AUC': 0.8445491239755181, 'AP': 0.9382704997967297, 'EER': np.float64(0.2450892857142857)}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== FF++ Evaluation (TEST SET) =====\")\n",
    "\n",
    "# Use FF++ test split for evaluation\n",
    "FFPP_REAL_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\FFPP_CViT\\\\test\\\\real\"\n",
    "FFPP_FAKE_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\FFPP_CViT\\\\test\\\\fake\"\n",
    "\n",
    "ffpp_test_loader = DataLoader(\n",
    "    ImageDataset(FFPP_REAL_PATH, FFPP_FAKE_PATH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "metrics = evaluate_model(ffpp_test_loader)\n",
    "\n",
    "print(\"FF++ Test Metrics:\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6e7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Cross Dataset Evaluation =====\n",
      "\n",
      "===== Cross Dataset Evaluation =====\n",
      "CelebDF: {'ACC': 0.8998505852079356, 'AUC': 0.5775024575117633, 'AP': 0.9051846860594184, 'EER': np.float64(0.4451345755693582)}\n",
      "DFDC: {'ACC': 0.7794529743322004, 'AUC': 0.5398191363815406, 'AP': 0.8218131207698823, 'EER': np.float64(0.43862022319918836)}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== Cross Dataset Evaluation =====\")\n",
    "\n",
    "CELEBDF_REAL_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\CelebDF_images\\\\train\\\\real\"\n",
    "CELEBDF_FAKE_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\CelebDF_images\\\\train\\\\fake\"\n",
    "\n",
    "# Use DFDC validation split for faster cross-dataset evaluation\n",
    "DFDC_REAL_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\DFDC\\\\validation\\\\real\"\n",
    "DFDC_FAKE_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\DFDC\\\\validation\\\\fake\"\n",
    "\n",
    "print(\"\\n===== Cross Dataset Evaluation =====\")\n",
    "\n",
    "# ===== CelebDF =====\n",
    "if CELEBDF_REAL_PATH is not None and CELEBDF_FAKE_PATH is not None:\n",
    "\n",
    "    celeb_loader = DataLoader(\n",
    "        ImageDataset(CELEBDF_REAL_PATH, CELEBDF_FAKE_PATH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_model(celeb_loader)\n",
    "    print(\"CelebDF:\", metrics)\n",
    "\n",
    "\n",
    "# ===== DFDC =====\n",
    "if DFDC_REAL_PATH is not None and DFDC_FAKE_PATH is not None:\n",
    "\n",
    "    dfdc_loader = DataLoader(\n",
    "        ImageDataset(DFDC_REAL_PATH, DFDC_FAKE_PATH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_model(dfdc_loader)\n",
    "    print(\"DFDC:\", metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e0bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== JPEG Robustness Evaluation =====\n",
      "JPEG Quality 90 Metrics:\n",
      "{'ACC': 0.7465492000418279, 'AUC': 0.8438919521397943, 'AP': 0.9379180947162962, 'EER': np.float64(0.24732142857142858)}\n",
      "JPEG Quality 70 Metrics:\n",
      "{'ACC': 0.7450067970302207, 'AUC': 0.8422462030797828, 'AP': 0.9371441674328989, 'EER': np.float64(0.24803571428571428)}\n",
      "JPEG Quality 50 Metrics:\n",
      "{'ACC': 0.7439349576492732, 'AUC': 0.8402491431845547, 'AP': 0.9361300484228315, 'EER': np.float64(0.2517857142857143)}\n",
      "JPEG Quality 30 Metrics:\n",
      "{'ACC': 0.7450329394541462, 'AUC': 0.8351262845630638, 'AP': 0.9334116056080912, 'EER': np.float64(0.2588392857142857)}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== JPEG Robustness Evaluation (FF++ TEST) =====\")\n",
    "\n",
    "# Ensure we use FF++ test split here as well\n",
    "FFPP_REAL_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\FFPP_CViT\\\\test\\\\real\"\n",
    "FFPP_FAKE_PATH = r\"C:\\\\Users\\\\vk200\\\\OneDrive\\\\Desktop\\\\Benchmarking\\\\FFPP_CViT\\\\test\\\\fake\"\n",
    "\n",
    "jpeg_qualities = [90, 70, 50, 30]\n",
    "\n",
    "for q in jpeg_qualities:\n",
    "\n",
    "    jpeg_loader = DataLoader(\n",
    "        ImageDataset(\n",
    "            FFPP_REAL_PATH,\n",
    "            FFPP_FAKE_PATH,\n",
    "            jpeg_quality=q\n",
    "        ),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_model(jpeg_loader)\n",
    "\n",
    "    print(f\"JPEG Quality {q} Metrics:\")\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab12683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
