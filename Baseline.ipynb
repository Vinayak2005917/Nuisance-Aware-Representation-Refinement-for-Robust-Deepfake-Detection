{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ece8cb",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601b6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed08234",
   "metadata": {},
   "source": [
    "## 2. Config & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "975076f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    SEED = 42\n",
    "    IMG_SIZE = 224\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_WORKERS = 0\n",
    "    LR = 1e-4\n",
    "\n",
    "    DATA_ROOT = \"FFPP_CViT\"\n",
    "    WEIGHTS_DIR = \"weights\"\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(CFG.SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98201e",
   "metadata": {},
   "source": [
    "## 3. Dataset (Unified Binary Folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for label, cls in enumerate([\"real\", \"fake\"]):\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            for fname in os.listdir(cls_dir):\n",
    "                if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.samples.append(\n",
    "                        (os.path.join(cls_dir, fname), label)\n",
    "                    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d15463",
   "metadata": {},
   "source": [
    "## 4. JPEG Compression & Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c432f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JPEGCompression:\n",
    "    def __init__(self, quality):\n",
    "        self.quality = quality\n",
    "\n",
    "    def __call__(self, img):\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"JPEG\", quality=self.quality)\n",
    "        buffer.seek(0)\n",
    "        return Image.open(buffer).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e60e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "    transforms.RandomAffine(2, translate=(0.02, 0.02), scale=(0.95, 1.05), shear=2),\n",
    "    transforms.ColorJitter(0.6, 0.6, 0.6, 0.15),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(3, sigma=(0.1, 2.0))\n",
    "    ], p=0.3),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAdjustSharpness(0.5)\n",
    "    ], p=0.3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "def build_jpeg_tfms(q):\n",
    "    return transforms.Compose([\n",
    "        JPEGCompression(q),\n",
    "        transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c96a9",
   "metadata": {},
   "source": [
    "## 5. Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e84b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = models.resnet34(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.out_channels = 512\n",
    "\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47bfb8e",
   "metadata": {},
   "source": [
    "## 6. Tokenization & Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f6b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHead(nn.Module):\n",
    "    def __init__(self, in_channels, embed_dim=256, num_tokens=8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, 1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((num_tokens, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = self.pool(x)\n",
    "        return x.squeeze(-1).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bae12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=512,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, 2)\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x).mean(dim=1)\n",
    "        return self.fc(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc79db",
   "metadata": {},
   "source": [
    "## 7. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc0125f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = CNNBackbone()\n",
    "        self.embedder = EmbeddingHead(self.backbone.out_channels)\n",
    "        self.classifier = TokenClassifier(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        tokens = self.embedder(f)\n",
    "        logits = self.classifier(tokens)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d74cde",
   "metadata": {},
   "source": [
    "## 8. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47bdba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vk200\\OneDrive\\Desktop\\Benchmarking\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "model = DeepfakeDetector().to(device)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": model.backbone.parameters(),  \"lr\": CFG.LR * 0.2},\n",
    "    {\"params\": model.embedder.parameters(), \"lr\": CFG.LR},\n",
    "    {\"params\": model.classifier.parameters(),\"lr\": CFG.LR},\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80762063",
   "metadata": {},
   "source": [
    "## 9. Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d15d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    return str(datetime.datetime.now().time())[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4672aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4694b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(loader, model, threshold=0.5):\n",
    "    model.eval()\n",
    "    logits, labels = [], []\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        x = x.to(device)\n",
    "        logits.append(model(x).cpu())\n",
    "        labels.append(y)\n",
    "\n",
    "    logits = torch.cat(logits).numpy()\n",
    "    labels = torch.cat(labels).numpy()\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"auc\": roc_auc_score(labels, probs),\n",
    "        \"precision\": precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74852d85",
   "metadata": {},
   "source": [
    "## 10. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d597d271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 38252 samples from FFPP_CViT\\train\n",
      "[Dataset] Loaded 13445 samples from FFPP_CViT\\val\n",
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    }
   ],
   "source": [
    "train_ds = BinaryImageFolder(os.path.join(CFG.DATA_ROOT, \"train\"), train_tfms)\n",
    "val_ds   = BinaryImageFolder(os.path.join(CFG.DATA_ROOT, \"val\"),   eval_tfms)\n",
    "test_ds  = BinaryImageFolder(os.path.join(CFG.DATA_ROOT, \"test\"),  eval_tfms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, CFG.BATCH_SIZE, True,  num_workers=CFG.NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_ds,   CFG.BATCH_SIZE, False, num_workers=CFG.NUM_WORKERS)\n",
    "test_loader  = DataLoader(test_ds,  CFG.BATCH_SIZE, False, num_workers=CFG.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021a220",
   "metadata": {},
   "source": [
    "## 11. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3359c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 0.1270 | Val Acc: 0.8597 | AUC: 0.9337 | EMA-AUC: 0.9337 | P: 0.9587 | R: 0.8665 | F1: 0.9103 | Time: 08:03:24\n",
      "  ✓ Saved new best model (EMA-AUC=0.9337)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Loss: 0.1196 | Val Acc: 0.8763 | AUC: 0.9255 | EMA-AUC: 0.9320 | P: 0.9342 | R: 0.9138 | F1: 0.9239 | Time: 08:14:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Loss: 0.1011 | Val Acc: 0.8582 | AUC: 0.9309 | EMA-AUC: 0.9318 | P: 0.9587 | R: 0.8646 | F1: 0.9092 | Time: 08:26:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Loss: 0.0842 | Val Acc: 0.8659 | AUC: 0.9352 | EMA-AUC: 0.9325 | P: 0.9598 | R: 0.8733 | F1: 0.9145 | Time: 08:39:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Loss: 0.0604 | Val Acc: 0.8709 | AUC: 0.9366 | EMA-AUC: 0.9333 | P: 0.9566 | R: 0.8828 | F1: 0.9183 | Time: 08:51:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "os.makedirs(CFG.WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "ema_auc = None\n",
    "ema_decay = 0.8\n",
    "best_ema_auc = -1.0\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = train_epoch(train_loader, model, optimizer)\n",
    "    val_metrics = evaluate(val_loader, model)\n",
    "\n",
    "    current_auc = val_metrics[\"auc\"]\n",
    "\n",
    "    if ema_auc is None:\n",
    "        ema_auc = current_auc\n",
    "    else:\n",
    "        ema_auc = ema_decay * ema_auc + (1 - ema_decay) * current_auc\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Loss: {avg_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_metrics['acc']:.4f} | \"\n",
    "        f\"AUC: {current_auc:.4f} | \"\n",
    "        f\"EMA-AUC: {ema_auc:.4f} | \"\n",
    "        f\"P: {val_metrics['precision']:.4f} | \"\n",
    "        f\"R: {val_metrics['recall']:.4f} | \"\n",
    "        f\"F1: {val_metrics['f1']:.4f} | \"\n",
    "        f\"Time: {now()}\"\n",
    "    )\n",
    "\n",
    "    if ema_auc > best_ema_auc:\n",
    "        best_ema_auc = ema_auc\n",
    "        torch.save(model.state_dict(), f\"{CFG.WEIGHTS_DIR}/best_Baseline.pt\")\n",
    "        print(f\"  ✓ Saved new best model (EMA-AUC={best_ema_auc:.4f})\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a144f1",
   "metadata": {},
   "source": [
    "## 12. FF++ Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a5bf5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vk200\\AppData\\Local\\Temp\\ipykernel_6476\\2881336639.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f\"{CFG.WEIGHTS_DIR}/best_Baseline.pt\", map_location=device)\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FF++ TEST (BASELINE) | AVERAGED OVER 3 RUNS =====\n",
      "       ACC: 0.8667\n",
      "       AUC: 0.9371\n",
      " PRECISION: 0.9667\n",
      "    RECALL: 0.8678\n",
      "        F1: 0.9146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(f\"{CFG.WEIGHTS_DIR}/best_Baseline.pt\", map_location=device)\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "NUM_RUNS = 3\n",
    "all_metrics = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(NUM_RUNS):\n",
    "        set_seed(CFG.SEED + i)\n",
    "        test_metrics = evaluate(test_loader, model)\n",
    "        all_metrics.append(test_metrics)\n",
    "\n",
    "# Average metrics\n",
    "avg_metrics = {}\n",
    "for key in all_metrics[0].keys():\n",
    "    avg_metrics[key] = sum(m[key] for m in all_metrics) / NUM_RUNS\n",
    "\n",
    "print(\"\\n===== FF++ TEST (BASELINE) | AVERAGED OVER 3 RUNS =====\")\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k.upper():>10}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3aad73",
   "metadata": {},
   "source": [
    "## 13. JPEG Compression Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49c6021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== JPEG ROBUSTNESS (BASELINE | 3-RUN AVG) =====\n",
      "\n",
      "--- JPEG 100 ---\n",
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9370 | ACC: 0.8696 | F1: 0.9169\n",
      "\n",
      "--- JPEG 90 ---\n",
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9272 | ACC: 0.8665 | F1: 0.9156\n",
      "\n",
      "--- JPEG 75 ---\n",
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9194 | ACC: 0.8151 | F1: 0.8771\n",
      "\n",
      "--- JPEG 50 ---\n",
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8909 | ACC: 0.7386 | F1: 0.8156\n",
      "\n",
      "--- JPEG 30 ---\n",
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 13551 samples from FFPP_CViT\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8592 | ACC: 0.6539 | F1: 0.7397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== JPEG ROBUSTNESS (BASELINE | 3-RUN AVG) =====\")\n",
    "\n",
    "NUM_RUNS = 3\n",
    "jpeg_qualities = [100, 90, 75, 50, 30]\n",
    "\n",
    "for q in jpeg_qualities:\n",
    "    print(f\"\\n--- JPEG {q} ---\")\n",
    "\n",
    "    run_metrics = []\n",
    "\n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        set_seed(CFG.SEED + run_idx)\n",
    "        jpeg_ds = BinaryImageFolder(\n",
    "            os.path.join(CFG.DATA_ROOT, \"test\"),\n",
    "            build_jpeg_tfms(q)\n",
    "        )\n",
    "\n",
    "        jpeg_loader = DataLoader(\n",
    "            jpeg_ds,\n",
    "            CFG.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "        metrics = evaluate(jpeg_loader, model)\n",
    "        run_metrics.append(metrics)\n",
    "\n",
    "    avg_auc = sum(m[\"auc\"] for m in run_metrics) / NUM_RUNS\n",
    "    avg_acc = sum(m[\"acc\"] for m in run_metrics) / NUM_RUNS\n",
    "    avg_f1  = sum(m[\"f1\"]  for m in run_metrics) / NUM_RUNS\n",
    "\n",
    "    print(samples from\n",
    "        f\"AUC: {avg_auc:.4f} | \"\n",
    "        f\"ACC: {avg_acc:.4f} | \"\n",
    "        f\"F1: {avg_f1:.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3102e",
   "metadata": {},
   "source": [
    "## 14. DFDC Cross-Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9214ff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DFDC CROSS-DATASET (BASELINE | 3-RUN AVG) =====\n",
      "[Dataset] Loaded 30794 samples from ./DFDC/validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ACC: 0.7125\n",
      "       AUC: 0.7119\n",
      " PRECISION: 0.8586\n",
      "    RECALL: 0.7692\n",
      "        F1: 0.8114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== DFDC CROSS-DATASET (BASELINE | 3-RUN AVG) =====\")\n",
    "\n",
    "NUM_RUNS = 3\n",
    "DFDC_ROOT = \"./DFDC/validation\"\n",
    "\n",
    "dfdc_ds = BinaryImageFolder(DFDC_ROOT, eval_tfms)\n",
    "dfdc_loader = DataLoader(\n",
    "    dfdc_ds,\n",
    "    CFG.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for run_idx in range(NUM_RUNS):\n",
    "    set_seed(CFG.SEED + run_idx)\n",
    "    metrics = evaluate(dfdc_loader, model, threshold=0.5)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    k: sum(m[k] for m in all_metrics) / NUM_RUNS\n",
    "    for k in all_metrics[0].keys()\n",
    "}\n",
    "\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k.upper():>10}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fc414",
   "metadata": {},
   "source": [
    "## 15. CelebDF Cross-Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a82db88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CELEB-DF CROSS-DATASET (BASELINE | 3-RUN AVG) =====\n",
      "[Dataset] Loaded 8285 samples from ./CelebDF_images/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ACC: 0.6159\n",
      "       AUC: 0.6602\n",
      " PRECISION: 0.7504\n",
      "    RECALL: 0.6219\n",
      "        F1: 0.6801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== CELEB-DF CROSS-DATASET (BASELINE | 3-RUN AVG) =====\")\n",
    "\n",
    "NUM_RUNS = 3\n",
    "CELEBDF_ROOT = \"./CelebDF_images/test\"\n",
    "\n",
    "celeb_ds = BinaryImageFolder(CELEBDF_ROOT, eval_tfms)\n",
    "celeb_loader = DataLoader(\n",
    "    celeb_ds,\n",
    "    CFG.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for run_idx in range(NUM_RUNS):\n",
    "    set_seed(CFG.SEED + run_idx)\n",
    "    metrics = evaluate(celeb_loader, model, threshold=0.5)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    k: sum(m[k] for m in all_metrics) / NUM_RUNS\n",
    "    for k in all_metrics[0].keys()\n",
    "}\n",
    "\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k.upper():>10}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8be3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
