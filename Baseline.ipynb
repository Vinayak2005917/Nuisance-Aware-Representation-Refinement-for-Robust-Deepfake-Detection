{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69fe05fa",
   "metadata": {},
   "source": [
    "# Baseline Deepfake Detector (Benchmark Notebook)\n",
    "\n",
    "This notebook is the **fair baseline** used alongside NARR: same backbone/tokenization/classifier training protocol family, but **without** the NARR module.\n",
    "\n",
    "**Run order (typical):**\n",
    "1. Imports + config\n",
    "2. Dataset / augmentations\n",
    "3. Model\n",
    "4. Training loop\n",
    "5. Evaluation blocks (FF++, JPEG, DFDC, CelebDF)\n",
    "6. Params/FLOPs report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ece8cb",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "601b6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed08234",
   "metadata": {},
   "source": [
    "## 2. Config & Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01474286",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- `CFG.DATA_ROOT` should point at a folder with `train/`, `val/`, `test/` and each split containing `real/` + `fake/`.\n",
    "- `CFG.WEIGHTS_DIR` is where checkpoints are saved/loaded from.\n",
    "- Evaluation blocks use `NUM_RUNS` to average metrics over multiple seeded runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "975076f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    SEED = 42\n",
    "    IMG_SIZE = 224\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_WORKERS = 0\n",
    "    LR = 1e-4\n",
    "    \n",
    "    LAMBDA_INV = 0.05\n",
    "    LAMBDA_DOM = 0.1\n",
    "\n",
    "    DATA_ROOT = \"FFPP_CViT\"\n",
    "    WEIGHTS_DIR = \"weights\"\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(CFG.SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98201e",
   "metadata": {},
   "source": [
    "## 3. Dataset (Unified Binary Folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9b3928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for label, cls in enumerate([\"real\", \"fake\"]):\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            for fname in os.listdir(cls_dir):\n",
    "                if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.samples.append(\n",
    "                        (os.path.join(cls_dir, fname), label)\n",
    "                    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d15463",
   "metadata": {},
   "source": [
    "## 4. Standard Augmentations & Test Corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c432f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JPEGCompression:\n",
    "    def __init__(self, quality):\n",
    "        self.quality = quality\n",
    "\n",
    "    def __call__(self, img):\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"JPEG\", quality=self.quality)\n",
    "        buffer.seek(0)\n",
    "        return Image.open(buffer).convert(\"RGB\")\n",
    "\n",
    "\n",
    "class RandomGamma:\n",
    "    def __init__(self, gamma_range=(0.7, 1.5), p=0.5):\n",
    "        self.gamma_range = gamma_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            gamma = random.uniform(*self.gamma_range)\n",
    "            return transforms.functional.adjust_gamma(img, gamma)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e60e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "    transforms.RandomAffine(2, translate=(0.02, 0.02), scale=(0.95, 1.05), shear=2),\n",
    "    transforms.ColorJitter(0.6, 0.6, 0.6, 0.15),\n",
    "    transforms.RandomApply([\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))\n",
    "    ], p=0.3),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    RandomGamma(p=0.5),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAdjustSharpness(0.5)\n",
    "    ], p=0.3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "def build_jpeg_tfms(q):\n",
    "    return transforms.Compose([\n",
    "        JPEGCompression(q),\n",
    "        transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88399d",
   "metadata": {},
   "source": [
    "## 4.5 Corruption Functions (Training Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b464cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_image(x):\n",
    "    out = x.clone()\n",
    "\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        out = F.interpolate(out, scale_factor=0.75, mode=\"bilinear\", align_corners=False)\n",
    "        out = F.interpolate(out, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        out = torch.clamp(out + 0.03 * torch.randn_like(out), 0, 1)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def freq_mix(x, alpha=0.15):\n",
    "    fft = torch.fft.fft2(x)\n",
    "    mag, phase = torch.abs(fft), torch.angle(fft)\n",
    "    mag = mag * (1 + alpha * torch.randn_like(mag))\n",
    "    return torch.real(torch.fft.ifft2(mag * torch.exp(1j * phase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c96a9",
   "metadata": {},
   "source": [
    "## 5. Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36e84b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = models.resnet34(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.out_channels = 512\n",
    "\n",
    "        for m in self.features.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "\n",
    "class GradReverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, λ):\n",
    "        ctx.λ = λ\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        return -ctx.λ * grad, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47bfb8e",
   "metadata": {},
   "source": [
    "## 6. Tokenization & Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b1f6b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHead(nn.Module):\n",
    "    def __init__(self, in_channels, embed_dim=256, num_tokens=8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, 1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((num_tokens, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = self.pool(x)\n",
    "        return x.squeeze(-1).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0bae12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=512,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, 2)\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x).mean(dim=1)\n",
    "        return self.fc(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc79db",
   "metadata": {},
   "source": [
    "## 7. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc0125f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyDomainHead(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lambda_grl=0.0):\n",
    "        if lambda_grl > 0:\n",
    "            x = GradReverse.apply(x, lambda_grl)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class DeepfakeDetectorBaselinePP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = CNNBackbone()\n",
    "        self.domain_head = DummyDomainHead(self.backbone.out_channels)\n",
    "        self.embedder = EmbeddingHead(self.backbone.out_channels)\n",
    "        self.classifier = TokenClassifier(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        tokens = self.embedder(f)\n",
    "        return self.classifier(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f1ab8",
   "metadata": {},
   "source": [
    "### Fair Baseline Protocol\n",
    "This baseline keeps the same training protocol family as NARR (classification + invariance + domain adversarial), but removes the NARR module itself.\n",
    "- No nuisance estimator / suppression block\n",
    "- Backbone, tokenizer, and classifier remain\n",
    "- Domain adversarial control head kept for protocol parity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d74cde",
   "metadata": {},
   "source": [
    "## 8. Training Setup (Fair Baseline: NARR Removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47bdba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "model = DeepfakeDetectorBaselinePP().to(device)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": model.backbone.parameters(),  \"lr\": CFG.LR * 0.2},\n",
    "    {\"params\": model.domain_head.parameters(),\"lr\": CFG.LR},\n",
    "    {\"params\": model.embedder.parameters(),  \"lr\": CFG.LR},\n",
    "    {\"params\": model.classifier.parameters(),\"lr\": CFG.LR},\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=EPOCHS\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "domain_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def invariance_contrastive_loss(z1, z2, temp=0.2):\n",
    "    z1 = F.normalize(z1.mean(1), dim=1)\n",
    "    z2 = F.normalize(z2.mean(1), dim=1)\n",
    "    logits = (z1 @ z2.T / temp).clamp(-50, 50)\n",
    "    labels = torch.arange(z1.size(0), device=z1.device)\n",
    "    return F.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80762063",
   "metadata": {},
   "source": [
    "## 9. Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87d15d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    return str(datetime.datetime.now().time())[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4672aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_baselinepp(loader, model, optimizer):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        f = model.backbone(x)\n",
    "        dom_clean = model.domain_head(f, lambda_grl=0.1)\n",
    "        tok_clean = model.embedder(f)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if torch.rand(1) < 0.5:\n",
    "                x_corr = corrupt_image(x)\n",
    "            else:\n",
    "                x_corr = freq_mix(x)\n",
    "\n",
    "        f_c = model.backbone(x_corr)\n",
    "        dom_corrupt = model.domain_head(f_c, lambda_grl=0.1)\n",
    "        tok_corr = model.embedder(f_c)\n",
    "\n",
    "        loss_inv = invariance_contrastive_loss(tok_clean, tok_corr)\n",
    "\n",
    "        logit = model.classifier(tok_clean)\n",
    "        loss_cls = criterion(logit, y)\n",
    "\n",
    "        dom_y_clean = torch.zeros(x.size(0), dtype=torch.long, device=device)\n",
    "        dom_y_corrupt = torch.ones(x.size(0), dtype=torch.long, device=device)\n",
    "\n",
    "        loss_dom = (\n",
    "            domain_criterion(dom_clean, dom_y_clean)\n",
    "            + domain_criterion(dom_corrupt, dom_y_corrupt)\n",
    "        )\n",
    "\n",
    "        loss = loss_cls + CFG.LAMBDA_INV * loss_inv + CFG.LAMBDA_DOM * loss_dom\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item()\n",
    "\n",
    "    return total / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf4694b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(loader, model, threshold=0.5):\n",
    "    model.eval()\n",
    "    logits, labels = [], []\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        x = x.to(device)\n",
    "        logits.append(model(x).cpu())\n",
    "        labels.append(y)\n",
    "\n",
    "    logits = torch.cat(logits).numpy()\n",
    "    labels = torch.cat(labels).numpy()\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"auc\": roc_auc_score(labels, probs),\n",
    "        \"precision\": precision_score(labels, preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, zero_division=0),\n",
    "        \"f1\": f1_score(labels, preds, zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74852d85",
   "metadata": {},
   "source": [
    "## 10. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d597d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BinaryImageFolder(os.path.join(CFG.DATA_ROOT, \"train\"), train_tfms)\n",
    "val_ds   = BinaryImageFolder(os.path.join(CFG.DATA_ROOT, \"val\"),   eval_tfms)\n",
    "test_ds  = BinaryImageFolder(os.path.join(CFG.DATA_ROOT, \"test\"),  eval_tfms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, CFG.BATCH_SIZE, True,  num_workers=CFG.NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_ds,   CFG.BATCH_SIZE, False, num_workers=CFG.NUM_WORKERS)\n",
    "test_loader  = DataLoader(test_ds,  CFG.BATCH_SIZE, False, num_workers=CFG.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021a220",
   "metadata": {},
   "source": [
    "## 11. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d3359c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 0.5492 | Val Acc: 0.7489 | AUC: 0.8947 | P: 0.9648 | R: 0.7206 | F1: 0.8250 | Time: 13:26:19\n",
      "  ✓ Saved new best model (AUC=0.8947)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Loss: 0.4129 | Val Acc: 0.8167 | AUC: 0.9213 | P: 0.9661 | R: 0.8052 | F1: 0.8783 | Time: 13:43:18\n",
      "  ✓ Saved new best model (AUC=0.9213)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Loss: 0.3428 | Val Acc: 0.8470 | AUC: 0.9226 | P: 0.9607 | R: 0.8484 | F1: 0.9011 | Time: 13:59:48\n",
      "  ✓ Saved new best model (AUC=0.9226)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Loss: 0.2936 | Val Acc: 0.8440 | AUC: 0.9299 | P: 0.9639 | R: 0.8416 | F1: 0.8986 | Time: 14:15:33\n",
      "  ✓ Saved new best model (AUC=0.9299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Loss: 0.2572 | Val Acc: 0.8324 | AUC: 0.9232 | P: 0.9635 | R: 0.8273 | F1: 0.8903 | Time: 14:33:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "os.makedirs(CFG.WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "best_auc = -1.0\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = train_epoch_baselinepp(train_loader, model, optimizer)\n",
    "    val_metrics = evaluate(val_loader, model)\n",
    "    current_auc = val_metrics[\"auc\"]\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Loss: {avg_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_metrics['acc']:.4f} | \"\n",
    "        f\"AUC: {current_auc:.4f} | \"\n",
    "        f\"P: {val_metrics['precision']:.4f} | \"\n",
    "        f\"R: {val_metrics['recall']:.4f} | \"\n",
    "        f\"F1: {val_metrics['f1']:.4f} | \"\n",
    "        f\"Time: {now()}\"\n",
    "    )\n",
    "\n",
    "    if current_auc > best_auc:\n",
    "        best_auc = current_auc\n",
    "        torch.save(model.state_dict(), f\"{CFG.WEIGHTS_DIR}/best_Baseline.pt\")\n",
    "        print(f\"  ✓ Saved new best model (AUC={best_auc:.4f})\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a144f1",
   "metadata": {},
   "source": [
    "## 12. FF++ Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bf5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vk200\\AppData\\Local\\Temp\\ipykernel_8024\\1628477025.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f\"{CFG.WEIGHTS_DIR}/best_Baseline.pt\", map_location=device)\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FF++ TEST (BASELINE) | AVERAGED OVER 3 RUNS =====\n",
      "       ACC: 0.8455\n",
      "       AUC: 0.9331\n",
      " PRECISION: 0.9698\n",
      "    RECALL: 0.8383\n",
      "        F1: 0.8993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(f\"{CFG.WEIGHTS_DIR}/best_Baseline.pt\", map_location=device)\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Number of repeated evaluation runs (different seeds) to average metrics over.\n",
    "NUM_RUNS = 1\n",
    "all_metrics = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(NUM_RUNS):\n",
    "        set_seed(CFG.SEED + i)\n",
    "        test_metrics = evaluate(test_loader, model)\n",
    "        all_metrics.append(test_metrics)\n",
    "\n",
    "# Average metrics\n",
    "avg_metrics = {}\n",
    "for key in all_metrics[0].keys():\n",
    "    avg_metrics[key] = sum(m[key] for m in all_metrics) / NUM_RUNS\n",
    "\n",
    "print(f\"\\n===== FF++ TEST (BASELINE) | AVERAGED OVER {NUM_RUNS} RUN(S) =====\")\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k.upper():>10}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3aad73",
   "metadata": {},
   "source": [
    "## 13. JPEG Compression Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== JPEG ROBUSTNESS (BASELINE | 3-RUN AVG) =====\n",
      "\n",
      "--- JPEG 100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9331 | ACC: 0.8509 | F1: 0.9034\n",
      "\n",
      "--- JPEG 90 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9257 | ACC: 0.8656 | F1: 0.9147\n",
      "\n",
      "--- JPEG 75 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9147 | ACC: 0.8039 | F1: 0.8681\n",
      "\n",
      "--- JPEG 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8811 | ACC: 0.7332 | F1: 0.8120\n",
      "\n",
      "--- JPEG 30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8467 | ACC: 0.6794 | F1: 0.7651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(f\"\\n===== JPEG ROBUSTNESS (BASELINE | {NUM_RUNS}-RUN AVG) =====\")\n",
    "\n",
    "jpeg_qualities = [100, 90, 75, 50, 30]\n",
    "\n",
    "for q in jpeg_qualities:\n",
    "    print(f\"\\n--- JPEG {q} ---\")\n",
    "\n",
    "    run_metrics = []\n",
    "\n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        set_seed(CFG.SEED + run_idx)\n",
    "        jpeg_ds = BinaryImageFolder(\n",
    "            os.path.join(CFG.DATA_ROOT, \"test\"),\n",
    "            build_jpeg_tfms(q)\n",
    ")\n",
    "\n",
    "        jpeg_loader = DataLoader(\n",
    "            jpeg_ds,\n",
    "            CFG.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "        metrics = evaluate(jpeg_loader, model)\n",
    "        run_metrics.append(metrics)\n",
    "\n",
    "    avg_auc = sum(m[\"auc\"] for m in run_metrics) / NUM_RUNS\n",
    "    avg_acc = sum(m[\"acc\"] for m in run_metrics) / NUM_RUNS\n",
    "    avg_f1  = sum(m[\"f1\"]  for m in run_metrics) / NUM_RUNS\n",
    "\n",
    "    print(\n",
    "        f\"AUC: {avg_auc:.4f} | \"\n",
    "        f\"ACC: {avg_acc:.4f} | \"\n",
    "        f\"F1: {avg_f1:.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3102e",
   "metadata": {},
   "source": [
    "## 14. DFDC Cross-Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214ff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DFDC CROSS-DATASET (BASELINE | 3-RUN AVG) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ACC: 0.7544\n",
      "       AUC: 0.6548\n",
      " PRECISION: 0.8342\n",
      "    RECALL: 0.8669\n",
      "        F1: 0.8502\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n===== DFDC CROSS-DATASET (BASELINE | {NUM_RUNS}-RUN AVG) =====\")\n",
    "\n",
    "DFDC_ROOT = \"./DFDC/validation\"\n",
    "\n",
    "dfdc_ds = BinaryImageFolder(DFDC_ROOT, eval_tfms)\n",
    "dfdc_loader = DataLoader(\n",
    "    dfdc_ds,\n",
    "    CFG.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for run_idx in range(NUM_RUNS):\n",
    "    set_seed(CFG.SEED + run_idx)\n",
    "    metrics = evaluate(dfdc_loader, model, threshold=0.5)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    k: sum(m[k] for m in all_metrics) / NUM_RUNS\n",
    "    for k in all_metrics[0].keys()\n",
    "}\n",
    "\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k.upper():>10}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fc414",
   "metadata": {},
   "source": [
    "## 15. CelebDF Cross-Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82db88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CELEB-DF CROSS-DATASET (BASELINE | 3-RUN AVG) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ACC: 0.6677\n",
      "       AUC: 0.6837\n",
      " PRECISION: 0.7524\n",
      "    RECALL: 0.7362\n",
      "        F1: 0.7442\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n===== CELEB-DF CROSS-DATASET (BASELINE | {NUM_RUNS}-RUN AVG) =====\")\n",
    "\n",
    "CELEBDF_ROOT = \"./CelebDF_images/test\"\n",
    "\n",
    "celeb_ds = BinaryImageFolder(CELEBDF_ROOT, eval_tfms)\n",
    "celeb_loader = DataLoader(\n",
    "    celeb_ds,\n",
    "    CFG.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS\n",
    ")\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for run_idx in range(NUM_RUNS):\n",
    "    set_seed(CFG.SEED + run_idx)\n",
    "    metrics = evaluate(celeb_loader, model, threshold=0.5)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    k: sum(m[k] for m in all_metrics) / NUM_RUNS\n",
    "    for k in all_metrics[0].keys()\n",
    "}\n",
    "\n",
    "for k, v in avg_metrics.items():\n",
    "    print(f\"{k.upper():>10}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# COMPUTE PARAMS + FLOPs FOR PAPER\n",
    "# =====================================\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "def compute_model_cost(model, name=\"Model\"):\n",
    "    \"\"\"Compute Params/FLOPs using a single dummy forward pass.\"\"\"\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # Dummy input (same as training resolution)\n",
    "    dummy = torch.randn(1, 3, CFG.IMG_SIZE, CFG.IMG_SIZE).to(device)\n",
    "\n",
    "    # Compute FLOPs + Params\n",
    "    flops, params = profile(\n",
    "        model,\n",
    "        inputs=(dummy,),\n",
    "        verbose=False\n",
    ")\n",
    "\n",
    "    # Pretty formatting (K, M, G)\n",
    "    flops_str, params_str = clever_format([flops, params], \"%.3f\")\n",
    "\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(f\"Params : {params_str}\")\n",
    "    print(f\"FLOPs  : {flops_str}\")\n",
    "\n",
    "    return flops, params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60118e06",
   "metadata": {},
   "source": [
    "## 16. Model Cost (Params + FLOPs)\n",
    "Uses `thop` to estimate Params/FLOPs with a dummy input at `CFG.IMG_SIZE`. Install once via: `pip install thop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505afda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Baseline Detector =====\n",
      "Params : 21.944M\n",
      "FLOPs  : 3.689G\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a fresh model for cost reporting (weights are not required for Params/FLOPs).\n",
    "baseline_model = DeepfakeDetectorBaselinePP().to(device)\n",
    "\n",
    "# Compute costs\n",
    "flops_base, params_base = compute_model_cost(\n",
    "    baseline_model,\n",
    "    name=\"Baseline Detector\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8024fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
